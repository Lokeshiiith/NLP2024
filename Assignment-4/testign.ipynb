{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# check for cuda\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "import argparse\n",
    "import ast\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy.sparse import lil_matrix\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "# check for cuda\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file and extract the Description column\n",
    "data = pd.read_csv('file/train.csv')\n",
    "corpus = data['Description']\n",
    "sentence_count= 0\n",
    "sentence_limit = 40\n",
    "sentences_list = []\n",
    "\n",
    "for sentence in corpus:\n",
    "    sentences_list.append(sentence)\n",
    "    sentence_count += 1\n",
    "    if sentence_count > sentence_limit:\n",
    "        break\n",
    "\n",
    "word_counts = {}\n",
    "text_courpus = []\n",
    "MinWords = 40000 # min words in a sentence\n",
    "minSentenceLength = 500 # min words in a sentence\n",
    "maxSenteceLength = 0 # max words in a sentence\n",
    "for sentence in sentences_list:\n",
    "    sentence = sentence.lower() # convert to lower case\n",
    "    sentence = re.sub(r'[^\\w\\s]', '', sentence) # remove punctuation\n",
    "    text_courpus.append(sentence)\n",
    "    words = sentence.split()\n",
    "    minSentenceLength = min(minSentenceLength, len(words))\n",
    "    maxSenteceLength = max(maxSenteceLength, len(words))\n",
    "    for word in words:\n",
    "        if word in word_counts:\n",
    "            word_counts[word] += 1\n",
    "        else:\n",
    "            word_counts[word] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordEmbedding:\n",
    "    def __init__(self, window_size=2, embedding_size=100):\n",
    "        self.window_size = window_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.word2index = {}\n",
    "        self.index2word = {}\n",
    "        self.word_counts = {}\n",
    "        self.word_vectors = None\n",
    "        self.word_vectors_norm = None\n",
    "        self.vocab_size = 0\n",
    "    def buid_Co_occurence_matrix(self, corpus, word_counts):\n",
    "        self.word2index = {word: i for i, word in enumerate(word_counts.keys())}\n",
    "        self.index2word = {i: word for word, i in self.word2index.items()}\n",
    "        self.word_counts = word_counts\n",
    "        self.corpus = corpus\n",
    "        vocab = set(word_counts.keys())\n",
    "        self.vocab_size = len(vocab)\n",
    "        #Create a n*n matrix where n is the number of words in the vocab\n",
    "        self.co_occurence_matrix = lil_matrix((self.vocab_size, self.vocab_size), dtype=np.float32)\n",
    "        for sentence in corpus:\n",
    "            words = sentence.split()\n",
    "            for i, word in enumerate(words):\n",
    "                if word in self.word2index:\n",
    "                    for j in range(max(i - self.window_size, 0), min(i + self.window_size, len(words))):\n",
    "                        if i != j and words[j] in self.word2index:\n",
    "                            self.co_occurence_matrix[self.word2index[word], self.word2index[words[j]]] += 1\n",
    "    \n",
    "    def train(self, corpus, word_counts):\n",
    "        self.buid_Co_occurence_matrix(corpus, word_counts)\n",
    "        svd = TruncatedSVD(n_components=self.embedding_size)\n",
    "        svd.fit(self.co_occurence_matrix)\n",
    "        embeddings = svd.components_\n",
    "        embeddings = np.transpose(embeddings)\n",
    "        return embeddings, self.word2index, self.index2word\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Co-occurrence Matrix:\n",
      "[[0. 1. 0. 0. 1. 1. 1. 0.]\n",
      " [1. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 1. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "\n",
      "Word Embeddings:\n",
      "the: [ 0.6592979 -0.6249202]\n",
      "quick: [0.40738988 0.2680209 ]\n",
      "brown: [ 0.18280017 -0.24515137]\n",
      "fox: [ 0.15605648 -0.1453273 ]\n",
      "jumps: [0.29821414 0.31788492]\n",
      "over: [0.3752575  0.50180185]\n",
      "lazy: [0.29821417 0.31788495]\n",
      "dog: [ 0.15135553 -0.05013293]\n"
     ]
    }
   ],
   "source": [
    "# Sample corpus and word counts\n",
    "corpus1 = [\n",
    "    \"the quick brown fox\",\n",
    "    \"jumps over the lazy dog\"\n",
    "]\n",
    "\n",
    "word_counts1 = {\n",
    "    \"the\": 2,\n",
    "    \"quick\": 1,\n",
    "    \"brown\": 1,\n",
    "    \"fox\": 1,\n",
    "    \"jumps\": 1,\n",
    "    \"over\": 1,\n",
    "    \"lazy\": 1,\n",
    "    \"dog\": 1\n",
    "}\n",
    "\n",
    "# Initialize and train the WordEmbedding model\n",
    "embedding_model = WordEmbedding(window_size=2, embedding_size=2)\n",
    "embeddings, word2index, index2word = embedding_model.train(corpus1, word_counts1)\n",
    "\n",
    "# Print the co-occurrence matrix\n",
    "print(\"Co-occurrence Matrix:\")\n",
    "print(embedding_model.co_occurence_matrix.toarray())\n",
    "\n",
    "# Print the resulting embeddings\n",
    "print(\"\\nWord Embeddings:\")\n",
    "for word, idx in word2index.items():\n",
    "    print(f\"{word}: {embeddings[idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 297 ms\n",
      "Wall time: 272 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Embedding_Model = WordEmbedding(window_size=3, embedding_size=100)\n",
    "embeddings, word2index, index2word = Embedding_Model.train(text_courpus, word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "word_embeddings_tensor = torch.FloatTensor(embeddings)\n",
    "# Save the embeddings to a file\n",
    "torch.save(word_embeddings_tensor, 'svd-word-vectors.pt')\n",
    "# Save word2index and index2word to JSON files\n",
    "def save_dictionary(dictionary, file_path):\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(dictionary, f)\n",
    "\n",
    "# Load word2index and index2word from JSON files\n",
    "\n",
    "# Save dictionaries to JSON files\n",
    "save_dictionary(word2index, 'word2index.json')\n",
    "save_dictionary(index2word, 'index2word.json')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "640"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elmo Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ELMO(nn.Module):\n",
    "    def __init__(self, initial_embedding, hidden_size, num_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize embedding layer\n",
    "        self.embedding_layer = nn.Embedding.from_pretrained(torch.tensor(initial_embedding))\n",
    "\n",
    "        # Initialize bidirectional LSTM layers\n",
    "        self.lstm_layer1 = nn.LSTM(input_size=self.embedding_layer.embedding_dim, \n",
    "                                   hidden_size=hidden_size, \n",
    "                                   bidirectional=True, \n",
    "                                   batch_first=True)\n",
    "        self.lstm_layer2 = nn.LSTM(input_size=self.embedding_layer.embedding_dim, \n",
    "                                   hidden_size=hidden_size, \n",
    "                                   bidirectional=True, \n",
    "                                   batch_first=True)\n",
    "        \n",
    "        # Initialize linear layer\n",
    "        self.linear_layer = nn.Linear(in_features=self.embedding_layer.embedding_dim, \n",
    "                                      out_features=self.embedding_layer.num_embeddings)\n",
    "\n",
    "        # Initialize lambda parameters\n",
    "        self.lambda1 = nn.Parameter(torch.rand(1))\n",
    "        self.lambda2 = nn.Parameter(torch.rand(1))\n",
    "        self.lambda3 = nn.Parameter(torch.rand(1))\n",
    "\n",
    "    def forward(self, x, is_training):\n",
    "        # Embedding layer\n",
    "        embedded_input = self.embedding_layer(x)\n",
    "        \n",
    "        # BiLSTM layers\n",
    "        lstm_output1, _ = self.lstm_layer1(embedded_input)\n",
    "        lstm_output2, _ = self.lstm_layer2(lstm_output1)\n",
    "        \n",
    "        # Weighted sum of embeddings\n",
    "        output = self.lambda1 * embedded_input + self.lambda2 * lstm_output1 + self.lambda3 * lstm_output2\n",
    "        \n",
    "        # Apply linear layer if training\n",
    "        if is_training:\n",
    "            output = self.linear_layer(output)\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_sentences(sentences):\n",
    "    shifted_sentences = []\n",
    "    for sentence in sentences:\n",
    "        # shifted_sentence = torch.cat((sentence[1:], torch.tensor([wordtoind['<eos>']])))\n",
    "        shifted_sentence = sentence[1:] + [word2index['<eos>']]\n",
    "        shifted_sentences.append(shifted_sentence)\n",
    "\n",
    "    return shifted_sentences"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
