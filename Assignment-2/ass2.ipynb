{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import conllu\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "  device = torch.device(\"cuda\")\n",
    "else:\n",
    "  device = torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = {}\n",
    "tag_to_index = {}\n",
    "max_sentence_length = 0\n",
    "word_count = {}\n",
    "word_to_index['<PAD>'] = 0\n",
    "tag_to_index['<UNK>'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Got all senteces with padding and each sentence with pos tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will provide each sentence with paddings <unk>\n",
    "def process_dataset(dataset_file, p=2, s=3):\n",
    "    sentences_list = []\n",
    "    pos_list = []\n",
    "\n",
    "    with open(dataset_file, 'r', encoding='utf-8') as f:\n",
    "        sentence_tokens = []\n",
    "        pos_tags = []\n",
    "\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "\n",
    "            if line.startswith('#'):\n",
    "                sentence_tokens = []\n",
    "                pos_tags = []\n",
    "                continue\n",
    "            elif line == '':\n",
    "                # Append padding to the end of the sentence\n",
    "                padded_sentence = ' '.join(['<PAD>'] * p) + ' ' + ' '.join(sentence_tokens) + ' ' + ' '.join(['<PAD>'] * s)\n",
    "                padded_pos = ' '.join(['<UNK>'] * p + pos_tags + ['<UNK>'] * s)\n",
    "                sentences_list.append(padded_sentence)\n",
    "                pos_list.append(padded_pos)\n",
    "                continue\n",
    "            else:\n",
    "                # New sentence begins\n",
    "                token_attrs = line.split('\\t')\n",
    "                word_form = token_attrs[1]  # Word form of the token\n",
    "                pos_tag = token_attrs[3]    # POS tag of the token\n",
    "                sentence_tokens.append(word_form)\n",
    "                pos_tags.append(pos_tag)\n",
    "\n",
    "    return sentences_list, pos_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indices(sentences_list, pos_list, word_to_index, tag_to_index, max_sentence_length, word_count):\n",
    "    # Process each sentence to tokenize the data\n",
    "    for sentence_str, tag_str in zip(sentences_list, pos_list):\n",
    "        # Tokenize the sentence into individual tokens\n",
    "        tokens = sentence_str.split(' ')\n",
    "        tags = tag_str.split(' ')\n",
    "            # Word to index\n",
    "        for word, tag in zip(tokens, tags):\n",
    "            if word not in word_to_index:\n",
    "                word_to_index[word] = len(word_to_index)\n",
    "            word_count[word] = word_count.get(word, 0) + 1\n",
    "        # Tag to index\n",
    "            if tag not in tag_to_index:\n",
    "                tag_to_index[tag] = len(tag_to_index)\n",
    "        max_sentence_length = max(max_sentence_length, len(tokens))\n",
    "    return word_to_index, tag_to_index, max_sentence_length, word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = \"./conllu/train.conllu\"\n",
    "test_dataset = \"./conllu/test.conllu\"\n",
    "val_dataset = \"./conllu/val.conllu\"\n",
    "train_sentence_list, train_pos_list = process_dataset(train_dataset, p= 2, s= 3)\n",
    "test_sentence_list, test_pos_list = process_dataset(test_dataset, p= 2, s= 3)\n",
    "val_sentence_list, val_pos_list = process_dataset(val_dataset, p= 2, s= 3)\n",
    "\n",
    "\n",
    "sentences_list = train_sentence_list + test_sentence_list + val_sentence_list\n",
    "pos_list = train_pos_list + test_pos_list + val_pos_list\n",
    "\n",
    "# Split the data into sentences\n",
    "word_to_index = {'<UNK>': 0}\n",
    "tag_to_index = {'<UNK>': 0}\n",
    "word_count = {'<UNK>': 1}\n",
    "max_sentence_length = 0\n",
    "# get all indices\n",
    "word_to_index, tag_to_index, max_sentence_length, word_count = get_indices(sentences_list, pos_list, word_to_index, tag_to_index, max_sentence_length, word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for calculating embedding for all sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrepareEmbedding(sentence_dataset, pos_dataset, word_to_index, tag_to_index, max_sentence_length, word_count):\n",
    "    token_embeddings = []\n",
    "    labels_embedding = []\n",
    "    for sentence_str, tag_str in zip(sentence_dataset, pos_dataset):\n",
    "        # Tokenize the sentence into individual tokens\n",
    "        tokens = sentence_str.split(' ')\n",
    "        tags = tag_str.split(' ')\n",
    "        one_sentence_token_embedding = []\n",
    "        one_sentence_pos_embedding = []\n",
    "        # Word to index\n",
    "        for word, tag in zip(tokens, tags):\n",
    "            if word in word_to_index:\n",
    "                if word_count[word] < 2:\n",
    "                    word_cur_idx = word_to_index['<UNK>']\n",
    "                else:\n",
    "                    word_cur_idx = word_to_index[word]\n",
    "            else:\n",
    "                word_cur_idx = word_to_index['<UNK>']\n",
    "            # Tag to index\n",
    "            if tag in tag_to_index:\n",
    "                tag_cur_idx = tag_to_index[tag]\n",
    "            else:\n",
    "                tag_cur_idx = tag_to_index['<UNK>']\n",
    "            one_sentence_token_embedding.append(word_cur_idx)\n",
    "            one_sentence_pos_embedding.append(tag_cur_idx)\n",
    "        # Pad sequences using PyTorch's pad_sequence function\n",
    "        # one_sentence_token_embedding.extend([0] * (max_sentence_length - len(one_sentence_token_embedding)))\n",
    "        # one_sentence_pos_embedding.extend([0] * (max_sentence_length - len(one_sentence_pos_embedding)))\n",
    "        token_embeddings.append(one_sentence_token_embedding)\n",
    "        labels_embedding.append(one_sentence_pos_embedding)\n",
    "    return token_embeddings, labels_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentence_embeddings, train_pos_embeddings = PrepareEmbedding(train_sentence_list, train_pos_list, word_to_index, tag_to_index, max_sentence_length, word_count)\n",
    "test_sentence_embeddings, test_pos_embeddings = PrepareEmbedding(test_sentence_list, test_pos_list, word_to_index, tag_to_index, max_sentence_length, word_count)\n",
    "val_sentence_embeddings, val_pos_embeddings = PrepareEmbedding(val_sentence_list, val_pos_list, word_to_index, tag_to_index, max_sentence_length, word_count)\n",
    "# It will save a lot of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentence_embeddings = train_sentence_embeddings[:10]\n",
    "train_pos_embeddings = train_pos_embeddings[:10]\n",
    "test_sentence_embeddings = test_sentence_embeddings[:10]\n",
    "test_pos_embeddings = test_pos_embeddings[:10]\n",
    "val_sentence_embeddings = val_sentence_embeddings[:10]\n",
    "val_pos_embeddings = val_pos_embeddings[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"out.txt\", \"w\") as f:\n",
    "    for word in train_sentence_embeddings:\n",
    "        f.write(f\"{word}\\n\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving json of dictinary\n",
    "import json\n",
    "with open('word_to_index.json', 'w') as f:\n",
    "    json.dump(word_to_index, f)\n",
    "with open('tag_to_index.json', 'w') as f:\n",
    "    json.dump(tag_to_index, f)\n",
    "with open('word_count.json', 'w') as f:\n",
    "    json.dump(word_count, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed Forward netword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Step 1: Define the Model\n",
    "class FFNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, output_size, p, s):\n",
    "        super(FFNN, self).__init__()\n",
    "        # Calculate the actual input size considering embedding dimensions\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.fc1 = nn.Linear((p + s + 1) *embedding_dim , hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten the input tensor\n",
    "        first = self.embedding(x)\n",
    "        first = first.view(-1)\n",
    "        out = self.fc1(first)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FFNNmodel_path = 'FFNNmodel.pth'\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), FFNNmodel_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Train Loss: 1.9634\n",
      "Epoch [1/10], Validation Loss: 3.0968\n",
      "Epoch [2/10], Train Loss: 0.9286\n",
      "Epoch [2/10], Validation Loss: 2.8092\n",
      "Epoch [3/10], Train Loss: 0.7599\n",
      "Epoch [3/10], Validation Loss: 2.8984\n",
      "Epoch [4/10], Train Loss: 0.5813\n",
      "Epoch [4/10], Validation Loss: 3.1660\n",
      "Epoch [5/10], Train Loss: 0.5628\n",
      "Epoch [5/10], Validation Loss: 3.5876\n",
      "Epoch [6/10], Train Loss: 0.5331\n",
      "Epoch [6/10], Validation Loss: 3.9433\n",
      "Epoch [7/10], Train Loss: 0.4856\n",
      "Epoch [7/10], Validation Loss: 3.9942\n",
      "Epoch [8/10], Train Loss: 0.5076\n",
      "Epoch [8/10], Validation Loss: 4.4048\n",
      "Epoch [9/10], Train Loss: 0.5241\n",
      "Epoch [9/10], Validation Loss: 4.8696\n"
     ]
    }
   ],
   "source": [
    "# Define the number of steps after which to print the loss and accuracy\n",
    "print_interval = 100\n",
    "\n",
    "# Step 2: Define Loss Function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Step 3: Instantiate Model\n",
    "vocab_size = len(word_to_index)\n",
    "embedding_dim = 100  # Example dimension, adjust as needed\n",
    "hidden_size = 64  # Example size, adjust as needed\n",
    "output_size = len(tag_to_index)\n",
    "p = 2\n",
    "s = 3\n",
    "model = FFNN(vocab_size, embedding_dim, hidden_size, output_size, p, s).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001) # Example optimizer, adjust as needed\n",
    "\n",
    "def train_model(model, criterion, optimizer, train_embeddings, train_pos_embeddings, print_interval, epoch):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    # Iterate over the training dataset\n",
    "    for token_indices, pos_indices in zip(train_embeddings, train_pos_embeddings):\n",
    "        token_indices = torch.LongTensor(token_indices).to(device)\n",
    "        pos_indices = torch.LongTensor(pos_indices).to(device)\n",
    "        # Create sliding window of size 6 and convert to tensors\n",
    "        for i in range(p, len(token_indices) - s):\n",
    "            window_tokens = token_indices[i-p:i+s+1]\n",
    "            # window_tokens_tensor = torch.LongTensor(window_tokens).to(device)\n",
    "            pos_tag = pos_indices[i]\n",
    "            # creating one hot encoding for the pos tag\n",
    "            # length should be the number of tags\n",
    "            pos_tag_tensor = torch.zeros(len(tag_to_index)).to(device)\n",
    "            pos_tag_tensor[pos_tag] = 1\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(window_tokens)  # Forward pass\n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, pos_tag_tensor)  # Compare outputs with true labels\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "    return running_loss / len(train_embeddings)\n",
    "\n",
    "def evaluate_model(model, criterion, val_embeddings, val_pos_embeddings):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Iterate over the validation dataset\n",
    "    with torch.no_grad():\n",
    "        for token_indices, pos_indices in zip(val_embeddings, val_pos_embeddings):\n",
    "            # Create sliding window of size 6 and convert to tensors\n",
    "            token_indices = torch.LongTensor(token_indices).to(device)\n",
    "            pos_indices = torch.LongTensor(pos_indices).to(device)\n",
    "            for i in range(p, len(token_indices) - s):\n",
    "                window_tokens = token_indices[i-p:i+s+1]\n",
    "                # window_tokens_tensor = torch.LongTensor(window_tokens).to(device)\n",
    "                pos_tag = pos_indices[i]\n",
    "                # creating one hot encoding for the pos tag\n",
    "                # length should be the number of tags\n",
    "                pos_tag_tensor = torch.zeros(len(tag_to_index)).to(device)\n",
    "                pos_tag_tensor[pos_tag] = 1\n",
    "                outputs = model(window_tokens)  # Forward pass\n",
    "                # Calculate loss\n",
    "                loss = criterion(outputs, pos_tag_tensor)  # Compare outputs with true labels\n",
    "                running_loss += loss.item()\n",
    "\n",
    "    return running_loss / len(val_embeddings)\n",
    "\n",
    "def test_model(model, criterion, test_embeddings, test_pos_embeddings):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Iterate over the test dataset\n",
    "    with torch.no_grad():\n",
    "        for token_indices, pos_indices in zip(test_embeddings, test_pos_embeddings):\n",
    "            # Create sliding window of size 6 and convert to tensors\n",
    "            token_indices = torch.LongTensor(token_indices).to(device)\n",
    "            pos_indices = torch.LongTensor(pos_indices).to(device)\n",
    "            for i in range(p, len(token_indices) - s):\n",
    "                window_tokens = token_indices[i-p:i+s+1]\n",
    "                # window_tokens_tensor = torch.LongTensor(window_tokens)\n",
    "                pos_tag = pos_indices[i]\n",
    "                # creating one hot encoding for the pos tag\n",
    "                # length should be the number of tags\n",
    "                pos_tag_tensor = torch.zeros(len(tag_to_index)).to(device)\n",
    "                pos_tag_tensor[pos_tag] = 1\n",
    "                outputs = model(window_tokens)  # Forward pass\n",
    "                # Calculate loss\n",
    "                loss = criterion(outputs, pos_tag_tensor)  # Compare outputs with true labels\n",
    "                running_loss += loss.item()\n",
    "\n",
    "    return running_loss / len(test_embeddings)\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 10\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    train_loss = train_model(model, criterion, optimizer, train_sentence_embeddings, train_pos_embeddings, print_interval, epoch+1)\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss:.4f}\")\n",
    "    train_losses.append(train_loss)\n",
    "    # Validation phase\n",
    "    val_loss = evaluate_model(model, criterion, val_sentence_embeddings, val_pos_embeddings)\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Validation Loss: {val_loss:.4f}\")\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "# Testing phase\n",
    "test_loss = test_model(model, criterion, test_sentence_embeddings, test_pos_embeddings)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Step 4: Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "where: PRON\n",
      "is: PRON\n",
      "the: DET\n",
      "beset: VERB\n",
      "place: PROPN\n",
      "to: ADP\n",
      "go: PROPN\n",
      "for: ADP\n",
      "a: DET\n",
      "vacation: NOUN\n"
     ]
    }
   ],
   "source": [
    "def evaluateFFNN(model, sentences, word_to_index, tag_to_index, device):\n",
    "    sentence_token = sentences.split(' ')\n",
    "    embedded_sentence = [word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in sentence_token]\n",
    "    embedded_sentence = torch.tensor(embedded_sentence, dtype=torch.long)\n",
    "    token = []\n",
    "    tag = []\n",
    "    for i in range(2, len(embedded_sentence) - 3):\n",
    "        input = embedded_sentence[i-2:i+3 + 1]\n",
    "        output = model(input)\n",
    "        token.append(sentence_token[i])\n",
    "        tag.append(list(tag_to_index.keys())[list(tag_to_index.values()).index(torch.argmax(output).item())])\n",
    "\n",
    "    for i in range(len(token)):\n",
    "        print(f\"{token[i]}: {tag[i]}\")\n",
    "\n",
    "\n",
    "sentence = \"where is the beset place to go for a vacation\"\n",
    "padded_sentence = ' '.join(['<PAD>'] * p) + ' ' + ''.join(sentence) + ' ' + ' '.join(['<PAD>'] * s)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = FFNN(vocab_size, embedding_dim, hidden_size, output_size, p, s)\n",
    "model_param = torch.load('FFNNmodel.pth') \n",
    "model.load_state_dict(model_param)\n",
    "\n",
    "evaluateFFNN(model, padded_sentence, word_to_index, tag_to_index, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'collections.OrderedDict' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[407], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m padded_sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<PAD>\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m p) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(sentence) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<PAD>\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m s)\n\u001b[0;32m      3\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mFFNNmodel.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(device)\n\u001b[0;32m      5\u001b[0m evaluateFFNN(model, padded_sentence, word_to_index, tag_to_index, device)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'collections.OrderedDict' object has no attribute 'to'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question2 : 2 Recurrent Neural Network POS Tagging\n",
    "Design and implement a model which uses Recurrent Neural Networks (Vanilla\n",
    "RNN, LSTM, or GRU) for POS Tagging. The model should take the embed\u0002dings for all tokens in a sentence and output the corresponding POS tags in\n",
    "sequence.\n",
    "- For Example: In the sentence \"An apple a day keeps the doctor away\",\n",
    " the model takes the embeddings for \n",
    "- [\"An\", \"apple\", \"a\", \"day\", \"keeps\", \"the\",\"doctor\", \"away\"] and\n",
    " outputs the POS tags for all the words in the sentence\n",
    "- [\"DET\", \"NOUN\", \"DET\", \"NOUN\", \"VERB\", \"DET\", \"NOUN\", \"ADV\"] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step1 : Count all words and postags and provide them a index value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Doing it for all 3 datasets train, validation and test-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for all train, test and validation dataset\n",
    "- I got all sentences tokens and respective pos-tag in form of sentence sepateted by space \n",
    "- Max sentece length, word count , word to index adn tag t index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"out.txt\", \"w\") as f:\n",
    "    for word in tag_to_index:\n",
    "        f.write(f\"{word}\\n\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# index to tag dictionary will be used for predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '<UNK>',\n",
       " 1: 'PRON',\n",
       " 2: 'AUX',\n",
       " 3: 'DET',\n",
       " 4: 'NOUN',\n",
       " 5: 'ADP',\n",
       " 6: 'PROPN',\n",
       " 7: 'VERB',\n",
       " 8: 'NUM',\n",
       " 9: 'ADJ',\n",
       " 10: 'CCONJ',\n",
       " 11: 'ADV',\n",
       " 12: 'PART',\n",
       " 13: 'INTJ',\n",
       " 14: 'SYM'}"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_tag = {v: k for k, v in tag_to_index.items()}\n",
    "index_to_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lstm for training as well as validting and testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim * 2, tagset_size)\n",
    "\n",
    "    def forward(self, input_sentence):\n",
    "        embeds = self.word_embeddings(input_sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(input_sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(input_sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentence_embeddings, train_pos_embeddings = PrepareEmbedding(train_sentence_list, train_pos_list, word_to_index, tag_to_index, max_sentence_length, word_count)\n",
    "test_sentence_embeddings, test_pos_embeddings = PrepareEmbedding(test_sentence_list, test_pos_list, word_to_index, tag_to_index, max_sentence_length, word_count)\n",
    "val_sentence_embeddings, val_pos_embeddings = PrepareEmbedding(val_sentence_list, val_pos_list, word_to_index, tag_to_index, max_sentence_length, word_count)\n",
    "# It will save a lot of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"out.txt\", \"w\") as f:\n",
    "    for word in val_sentence_embeddings:\n",
    "        f.write(f\"{word}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word_to_index)\n",
    "tagset_size = len(tag_to_index)\n",
    "embedding_dim = 100\n",
    "hidden_dim = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentence_embeddings = train_sentence_embeddings[:80]\n",
    "train_pos_embeddings = train_pos_embeddings[:80]\n",
    "test_sentence_embeddings = test_sentence_embeddings[:4]\n",
    "test_pos_embeddings = test_pos_embeddings[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(model):\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.1)   \n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    for epoch in range(10):\n",
    "        model.zero_grad()\n",
    "        # ?PrepareEmbedding\n",
    "        for sentence, tags in zip(train_sentence_embeddings, train_pos_embeddings):\n",
    "            sentence_in = torch.LongTensor(sentence)\n",
    "            targets = torch.LongTensor(tags)\n",
    "            tag_scores = model(sentence_in)\n",
    "            loss = loss_function(tag_scores, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMTagger(embedding_dim, hidden_dim, len(word_to_index), len(tag_to_index))\n",
    "trainModel(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testModel(model, test_sentence_embeddings, test_pos_embeddings):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # No need to track gradients during testing\n",
    "        for sentence, tags in zip(test_sentence_embeddings, test_pos_embeddings):\n",
    "            sentence_in = torch.LongTensor(sentence)\n",
    "            targets = torch.LongTensor(tags)\n",
    "            tag_scores = model(sentence_in)\n",
    "            _, predicted = torch.max(tag_scores, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "    accuracy = correct / total\n",
    "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 5.48%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0547945205479452"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testModel(model, test_sentence_embeddings, test_pos_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<UNK>': 0,\n",
       " '<PAD>': 1,\n",
       " 'what': 2,\n",
       " 'is': 3,\n",
       " 'the': 4,\n",
       " 'cost': 5,\n",
       " 'of': 6,\n",
       " 'a': 7,\n",
       " 'round': 8,\n",
       " 'trip': 9,\n",
       " 'flight': 10,\n",
       " 'from': 11,\n",
       " 'pittsburgh': 12,\n",
       " 'to': 13,\n",
       " 'atlanta': 14,\n",
       " 'beginning': 15,\n",
       " 'on': 16,\n",
       " 'april': 17,\n",
       " 'twenty': 18,\n",
       " 'fifth': 19,\n",
       " 'and': 20,\n",
       " 'returning': 21,\n",
       " 'may': 22,\n",
       " 'sixth': 23,\n",
       " 'now': 24,\n",
       " 'i': 25,\n",
       " 'need': 26,\n",
       " 'leaving': 27,\n",
       " 'fort': 28,\n",
       " 'worth': 29,\n",
       " 'arriving': 30,\n",
       " 'in': 31,\n",
       " 'denver': 32,\n",
       " 'no': 33,\n",
       " 'later': 34,\n",
       " 'than': 35,\n",
       " '2': 36,\n",
       " 'pm': 37,\n",
       " 'next': 38,\n",
       " 'monday': 39,\n",
       " 'fly': 40,\n",
       " 'kansas': 41,\n",
       " 'city': 42,\n",
       " 'chicago': 43,\n",
       " 'wednesday': 44,\n",
       " 'following': 45,\n",
       " 'day': 46,\n",
       " 'meaning': 47,\n",
       " 'meal': 48,\n",
       " 'code': 49,\n",
       " 's': 50,\n",
       " 'show': 51,\n",
       " 'me': 52,\n",
       " 'all': 53,\n",
       " 'flights': 54,\n",
       " 'which': 55,\n",
       " 'serve': 56,\n",
       " 'for': 57,\n",
       " 'after': 58,\n",
       " 'tomorrow': 59,\n",
       " 'us': 60,\n",
       " 'air': 61,\n",
       " 'list': 62,\n",
       " 'nonstop': 63,\n",
       " 'early': 64,\n",
       " 'tuesday': 65,\n",
       " 'morning': 66,\n",
       " 'dallas': 67,\n",
       " 'st.': 68,\n",
       " 'petersburg': 69,\n",
       " 'toronto': 70,\n",
       " 'that': 71,\n",
       " 'arrive': 72,\n",
       " 'listing': 73,\n",
       " 'new': 74,\n",
       " 'york': 75,\n",
       " 'montreal': 76,\n",
       " 'canada': 77,\n",
       " 'departing': 78,\n",
       " 'thursday': 79,\n",
       " 'american': 80,\n",
       " 'airlines': 81,\n",
       " 'ontario': 82,\n",
       " 'with': 83,\n",
       " 'stopover': 84,\n",
       " 'louis': 85,\n",
       " 'ground': 86,\n",
       " 'transportation': 87,\n",
       " 'houston': 88,\n",
       " 'afternoon': 89,\n",
       " 'schedule': 90,\n",
       " 'philadelphia': 91,\n",
       " 'san': 92,\n",
       " 'francisco': 93,\n",
       " 'evening': 94,\n",
       " 'diego': 95,\n",
       " 'layover': 96,\n",
       " 'washington': 97,\n",
       " 'dc': 98,\n",
       " 'are': 99,\n",
       " 'there': 100,\n",
       " 'any': 101,\n",
       " 'boston': 102,\n",
       " 'stop': 103,\n",
       " 'restrictions': 104,\n",
       " 'cheapest': 105,\n",
       " 'one': 106,\n",
       " 'way': 107,\n",
       " 'fare': 108,\n",
       " 'between': 109,\n",
       " 'oakland': 110,\n",
       " 'airfare': 111,\n",
       " '416': 112,\n",
       " 'dollars': 113,\n",
       " \"'s\": 114,\n",
       " 'restriction': 115,\n",
       " 'ap68': 116,\n",
       " 'california': 117,\n",
       " 'airports': 118,\n",
       " 'available': 119,\n",
       " 'texas': 120,\n",
       " 'airport': 121,\n",
       " 'closest': 122,\n",
       " 'nevada': 123,\n",
       " 'arizona': 124,\n",
       " 'actually': 125,\n",
       " 'las': 126,\n",
       " 'vegas': 127,\n",
       " 'burbank': 128,\n",
       " 'saturday': 129,\n",
       " 'two': 130,\n",
       " 'how': 131,\n",
       " 'many': 132,\n",
       " 'going': 133,\n",
       " 'july': 134,\n",
       " 'seventh': 135,\n",
       " 'would': 136,\n",
       " 'like': 137,\n",
       " 'an': 138,\n",
       " 'february': 139,\n",
       " 'eighth': 140,\n",
       " 'before': 141,\n",
       " '9': 142,\n",
       " 'am': 143,\n",
       " 'second': 144,\n",
       " 'late': 145,\n",
       " 'okay': 146,\n",
       " 'june': 147,\n",
       " 'first': 148,\n",
       " \"'d\": 149,\n",
       " 'go': 150,\n",
       " 'phoenix': 151,\n",
       " 'detroit': 152,\n",
       " 'milwaukee': 153,\n",
       " 'indianapolis': 154,\n",
       " 'does': 155,\n",
       " 'ls': 156,\n",
       " 'stand': 157,\n",
       " 'designate': 158,\n",
       " 'as': 159,\n",
       " 'baltimore': 160,\n",
       " '1115': 161,\n",
       " '1245': 162,\n",
       " 'miami': 163,\n",
       " 'daily': 164,\n",
       " '8': 165,\n",
       " 'trans': 166,\n",
       " 'world': 167,\n",
       " 'airline': 168,\n",
       " '1030': 169,\n",
       " '1130': 170,\n",
       " '5': 171,\n",
       " '730': 172,\n",
       " 'leave': 173,\n",
       " 'charlotte': 174,\n",
       " 'north': 175,\n",
       " 'carolina': 176,\n",
       " '4': 177,\n",
       " 'find': 178,\n",
       " 'newark': 179,\n",
       " 'jersey': 180,\n",
       " 'cleveland': 181,\n",
       " 'ohio': 182,\n",
       " 'do': 183,\n",
       " 'you': 184,\n",
       " 'have': 185,\n",
       " 'connect': 186,\n",
       " 'international': 187,\n",
       " 'minneapolis': 188,\n",
       " 'rental': 189,\n",
       " 'cars': 190,\n",
       " \"'ll\": 191,\n",
       " 'rent': 192,\n",
       " 'car': 193,\n",
       " 'sort': 194,\n",
       " 'near': 195,\n",
       " 'fine': 196,\n",
       " 'can': 197,\n",
       " 'give': 198,\n",
       " 'information': 199,\n",
       " 'downtown': 200,\n",
       " 'economy': 201,\n",
       " 'class': 202,\n",
       " 'fares': 203,\n",
       " 'december': 204,\n",
       " 'sixteenth': 205,\n",
       " 'codes': 206,\n",
       " 'belong': 207,\n",
       " 'coach': 208,\n",
       " 'night': 209,\n",
       " 'service': 210,\n",
       " 'november': 211,\n",
       " 'twelfth': 212,\n",
       " 'eleventh': 213,\n",
       " 'want': 214,\n",
       " 'know': 215,\n",
       " 'or': 216,\n",
       " '1': 217,\n",
       " \"o'clock\": 218,\n",
       " '3': 219,\n",
       " '6': 220,\n",
       " '10': 221,\n",
       " 'august': 222,\n",
       " 'display': 223,\n",
       " 'depart': 224,\n",
       " 'please': 225,\n",
       " 'snacks': 226,\n",
       " 'served': 227,\n",
       " 'tower': 228,\n",
       " 'types': 229,\n",
       " 'meals': 230,\n",
       " 'ever': 231,\n",
       " 'my': 232,\n",
       " 'options': 233,\n",
       " '382': 234,\n",
       " 'get': 235,\n",
       " 'bwi': 236,\n",
       " 'eastern': 237,\n",
       " '210': 238,\n",
       " 'delta': 239,\n",
       " '852': 240,\n",
       " 'latest': 241,\n",
       " 'return': 242,\n",
       " 'same': 243,\n",
       " 'back': 244,\n",
       " 'most': 245,\n",
       " 'hours': 246,\n",
       " 'take': 247,\n",
       " 'so': 248,\n",
       " 'when': 249,\n",
       " 'will': 250,\n",
       " 'maximum': 251,\n",
       " 'amount': 252,\n",
       " 'time': 253,\n",
       " 'still': 254,\n",
       " 'earliest': 255,\n",
       " 'departure': 256,\n",
       " 'be': 257,\n",
       " 'travel': 258,\n",
       " 'at': 259,\n",
       " 'around': 260,\n",
       " '7': 261,\n",
       " 'route': 262,\n",
       " 'lastest': 263,\n",
       " 'longest': 264,\n",
       " 'but': 265,\n",
       " 'possible': 266,\n",
       " 'only': 267,\n",
       " 'weekdays': 268,\n",
       " 'red': 269,\n",
       " 'eye': 270,\n",
       " 'los': 271,\n",
       " 'angeles': 272,\n",
       " 'ten': 273,\n",
       " 'people': 274,\n",
       " 'during': 275,\n",
       " 'week': 276,\n",
       " 'days': 277,\n",
       " 'out': 278,\n",
       " 'arrives': 279,\n",
       " 'salt': 280,\n",
       " 'lake': 281,\n",
       " 'cincinnati': 282,\n",
       " 'area': 283,\n",
       " 'explain': 284,\n",
       " 'ap': 285,\n",
       " '57': 286,\n",
       " '20': 287,\n",
       " 'mean': 288,\n",
       " '80': 289,\n",
       " 'twa': 290,\n",
       " '497766': 291,\n",
       " 'has': 292,\n",
       " 'stops': 293,\n",
       " 'friday': 294,\n",
       " '705': 295,\n",
       " 'number': 296,\n",
       " 'book': 297,\n",
       " 'least': 298,\n",
       " '813': 299,\n",
       " 'goes': 300,\n",
       " 'straight': 301,\n",
       " 'through': 302,\n",
       " 'without': 303,\n",
       " 'stopping': 304,\n",
       " 'another': 305,\n",
       " 'florida': 306,\n",
       " 'tell': 307,\n",
       " 'about': 308,\n",
       " 'by': 309,\n",
       " 'memphis': 310,\n",
       " 'tennessee': 311,\n",
       " 'noon': 312,\n",
       " '530': 313,\n",
       " 'off': 314,\n",
       " 'love': 315,\n",
       " 'field': 316,\n",
       " 'united': 317,\n",
       " 'la': 318,\n",
       " 'guardia': 319,\n",
       " 'jfk': 320,\n",
       " 'mco': 321,\n",
       " 'sfo': 322,\n",
       " '1991': 323,\n",
       " 'orlando': 324,\n",
       " 'lowest': 325,\n",
       " 'dfw': 326,\n",
       " 'ticket': 327,\n",
       " 'oak': 328,\n",
       " 'atl': 329,\n",
       " 'logan': 330,\n",
       " 'march': 331,\n",
       " 'numbers': 332,\n",
       " 'expensive': 333,\n",
       " 'continental': 334,\n",
       " 'leaves': 335,\n",
       " '1220': 336,\n",
       " 'seattle': 337,\n",
       " 'columbus': 338,\n",
       " 'minnesota': 339,\n",
       " 'those': 340,\n",
       " 'via': 341,\n",
       " 'rentals': 342,\n",
       " 'sunday': 343,\n",
       " 'rates': 344,\n",
       " 'costs': 345,\n",
       " 'limousine': 346,\n",
       " 'taxi': 347,\n",
       " 'operation': 348,\n",
       " 'ap80': 349,\n",
       " 'ap57': 350,\n",
       " 'ninth': 351,\n",
       " '12': 352,\n",
       " 'america': 353,\n",
       " 'west': 354,\n",
       " 'could': 355,\n",
       " 'fifteenth': 356,\n",
       " 'serves': 357,\n",
       " 'dinner': 358,\n",
       " 'provided': 359,\n",
       " 'cities': 360,\n",
       " 'where': 361,\n",
       " 'lester': 362,\n",
       " 'pearson': 363,\n",
       " 'canadian': 364,\n",
       " 'other': 365,\n",
       " 'earlier': 366,\n",
       " '1017': 367,\n",
       " 'northwest': 368,\n",
       " 'general': 369,\n",
       " 'mitchell': 370,\n",
       " 'located': 371,\n",
       " 'both': 372,\n",
       " 'nationair': 373,\n",
       " 'midwest': 374,\n",
       " 'express': 375,\n",
       " 'flies': 376,\n",
       " 'zone': 377,\n",
       " 'flying': 378,\n",
       " 'into': 379,\n",
       " 'much': 380,\n",
       " 'price': 381,\n",
       " 'it': 382,\n",
       " 'tacoma': 383,\n",
       " 'anywhere': 384,\n",
       " '1850': 385,\n",
       " 'midnight': 386,\n",
       " 'january': 387,\n",
       " '1992': 388,\n",
       " 'not': 389,\n",
       " 'exceeding': 390,\n",
       " '300': 391,\n",
       " 'tenth': 392,\n",
       " '1993': 393,\n",
       " '1505': 394,\n",
       " 'october': 395,\n",
       " '1994': 396,\n",
       " 'carries': 397,\n",
       " 'smallest': 398,\n",
       " 'passengers': 399,\n",
       " 'thirty': 400,\n",
       " 'third': 401,\n",
       " 'arrival': 402,\n",
       " 'schedules': 403,\n",
       " 'times': 404,\n",
       " 'your': 405,\n",
       " '269': 406,\n",
       " '428': 407,\n",
       " 'westchester': 408,\n",
       " 'county': 409,\n",
       " 'right': 410,\n",
       " 'september': 411,\n",
       " 'twentieth': 412,\n",
       " 'f28': 413,\n",
       " '755': 414,\n",
       " 'nights': 415,\n",
       " 'their': 416,\n",
       " 'prices': 417,\n",
       " '1039': 418,\n",
       " 'less': 419,\n",
       " '1100': 420,\n",
       " 'nashville': 421,\n",
       " 'again': 422,\n",
       " 'repeat': 423,\n",
       " 'make': 424,\n",
       " 'iah': 425,\n",
       " 'ord': 426,\n",
       " 'ewr': 427,\n",
       " 'dca': 428,\n",
       " 'cvg': 429,\n",
       " 'bna': 430,\n",
       " 'mci': 431,\n",
       " 'hou': 432,\n",
       " 'lga': 433,\n",
       " 'lax': 434,\n",
       " 'yyz': 435,\n",
       " 'bur': 436,\n",
       " 'long': 437,\n",
       " 'distance': 438,\n",
       " 'far': 439,\n",
       " 'paul': 440,\n",
       " 'miles': 441,\n",
       " 'name': 442,\n",
       " 'serviced': 443,\n",
       " 'regarding': 444,\n",
       " 'tampa': 445,\n",
       " 'names': 446,\n",
       " 'describe': 447,\n",
       " 'nineteenth': 448,\n",
       " 'seating': 449,\n",
       " 'capacity': 450,\n",
       " 'fourteenth': 451,\n",
       " 'aircraft': 452,\n",
       " 'largest': 453,\n",
       " 'plane': 454,\n",
       " 'eight': 455,\n",
       " 'sixteen': 456,\n",
       " 'departures': 457,\n",
       " 'seventeenth': 458,\n",
       " 'arrivals': 459,\n",
       " 'type': 460,\n",
       " 'greatest': 461,\n",
       " 'more': 462,\n",
       " 'business': 463,\n",
       " 'total': 464,\n",
       " 'instead': 465,\n",
       " 'besides': 466,\n",
       " 'turboprop': 467,\n",
       " '1059': 468,\n",
       " 'advertises': 469,\n",
       " 'having': 470,\n",
       " 'land': 471,\n",
       " 'various': 472,\n",
       " 'dulles': 473,\n",
       " 'boeing': 474,\n",
       " '767': 475,\n",
       " '466': 476,\n",
       " '329': 477,\n",
       " 'under': 478,\n",
       " '932': 479,\n",
       " '1000': 480,\n",
       " '200': 481,\n",
       " '124': 482,\n",
       " 'along': 483,\n",
       " 'equal': 484,\n",
       " '150': 485,\n",
       " 'each': 486,\n",
       " '400': 487,\n",
       " 'fit': 488,\n",
       " '72s': 489,\n",
       " 'airplane': 490,\n",
       " 'l1011': 491,\n",
       " 'hold': 492,\n",
       " '733': 493,\n",
       " 'airplanes': 494,\n",
       " 'uses': 495,\n",
       " '73s': 496,\n",
       " 'seats': 497,\n",
       " '734': 498,\n",
       " 'm80': 499,\n",
       " 'l10': 500,\n",
       " 'carried': 501,\n",
       " 'capacities': 502,\n",
       " '757': 503,\n",
       " 'planes': 504,\n",
       " 'd9s': 505,\n",
       " '100': 506,\n",
       " 'thrift': 507,\n",
       " 'level': 508,\n",
       " 'see': 509,\n",
       " 'thirtieth': 510,\n",
       " '505': 511,\n",
       " '163': 512,\n",
       " 'tonight': 513,\n",
       " 'connecting': 514,\n",
       " 'also': 515,\n",
       " 'making': 516,\n",
       " \"'m\": 517,\n",
       " 'looking': 518,\n",
       " 'hopefully': 519,\n",
       " 'makes': 520,\n",
       " 'yes': 521,\n",
       " 'breakfast': 522,\n",
       " 'direct': 523,\n",
       " 'itinerary': 524,\n",
       " 'departs': 525,\n",
       " '1940': 526,\n",
       " 'connects': 527,\n",
       " 'provide': 528,\n",
       " 'used': 529,\n",
       " 'including': 530,\n",
       " 'connections': 531,\n",
       " 'if': 532,\n",
       " 'either': 533,\n",
       " 'preferably': 534,\n",
       " 'local': 535,\n",
       " 'beach': 536,\n",
       " 'then': 537,\n",
       " 'mornings': 538,\n",
       " 'four': 539,\n",
       " 'combination': 540,\n",
       " 'thank': 541,\n",
       " 'using': 542,\n",
       " 'well': 543,\n",
       " 'run': 544,\n",
       " 'colorado': 545,\n",
       " 'fourth': 546,\n",
       " 'who': 547,\n",
       " 'sure': 548,\n",
       " 'determine': 549,\n",
       " 'use': 550,\n",
       " '1765': 551,\n",
       " 'lufthansa': 552,\n",
       " 'eighteenth': 553,\n",
       " 'f': 554,\n",
       " 'today': 555,\n",
       " 'come': 556,\n",
       " '320': 557,\n",
       " 'booking': 558,\n",
       " 'k': 559,\n",
       " 'classes': 560,\n",
       " 'yn': 561,\n",
       " 'j31': 562,\n",
       " 'different': 563,\n",
       " 'dh8': 564,\n",
       " 'minimum': 565,\n",
       " 'connection': 566,\n",
       " 'intercontinental': 567,\n",
       " 'last': 568,\n",
       " 'aa': 569,\n",
       " '459': 570,\n",
       " 'limousines': 571,\n",
       " 'services': 572,\n",
       " 'jose': 573,\n",
       " 'too': 574,\n",
       " 'train': 575,\n",
       " 'stapleton': 576,\n",
       " 'limo': 577,\n",
       " 'georgia': 578,\n",
       " 'pennsylvania': 579,\n",
       " 'utah': 580,\n",
       " 'missouri': 581,\n",
       " 'interested': 582,\n",
       " 'shortest': 583,\n",
       " 'quebec': 584,\n",
       " 'michigan': 585,\n",
       " 'indiana': 586,\n",
       " 'this': 587,\n",
       " 'wednesdays': 588,\n",
       " '82': 589,\n",
       " '139': 590,\n",
       " 'tickets': 591,\n",
       " 'sounds': 592,\n",
       " 'great': 593,\n",
       " 'let': 594,\n",
       " 'takeoffs': 595,\n",
       " 'landings': 596,\n",
       " 'grounds': 597,\n",
       " 'offer': 598,\n",
       " 'transport': 599,\n",
       " 'kind': 600,\n",
       " 'hi': 601,\n",
       " 'calling': 602,\n",
       " 'coming': 603,\n",
       " 'soon': 604,\n",
       " 'thereafter': 605,\n",
       " 'anything': 606,\n",
       " 'bring': 607,\n",
       " 'up': 608,\n",
       " 'y': 609,\n",
       " 'm': 610,\n",
       " 'difference': 611,\n",
       " 'q': 612,\n",
       " 'b': 613,\n",
       " 'qo': 614,\n",
       " 'qw': 615,\n",
       " 'qx': 616,\n",
       " 'fn': 617,\n",
       " 'qualify': 618,\n",
       " 'h': 619,\n",
       " 'basis': 620,\n",
       " 'bh': 621,\n",
       " 'offers': 622,\n",
       " 'included': 623,\n",
       " 'serving': 624,\n",
       " 'trying': 625,\n",
       " 'include': 626,\n",
       " 'whether': 627,\n",
       " 'offered': 628,\n",
       " 'ua': 629,\n",
       " '270': 630,\n",
       " 'being': 631,\n",
       " '747': 632,\n",
       " 'be1': 633,\n",
       " '737': 634,\n",
       " 'very': 635,\n",
       " 'working': 636,\n",
       " 'scenario': 637,\n",
       " 'three': 638,\n",
       " '727': 639,\n",
       " 'called': 640,\n",
       " 'dc10': 641,\n",
       " 'abbreviation': 642,\n",
       " 'd10': 643,\n",
       " 'includes': 644,\n",
       " '296': 645,\n",
       " 'should': 646,\n",
       " 'lunch': 647,\n",
       " '343': 648,\n",
       " 'travels': 649,\n",
       " 'snack': 650,\n",
       " 'supper': 651,\n",
       " '838': 652,\n",
       " '1110': 653,\n",
       " 'reaches': 654,\n",
       " 'sometime': 655,\n",
       " 'some': 656,\n",
       " 'reaching': 657,\n",
       " 'saturdays': 658,\n",
       " 'vicinity': 659,\n",
       " 'good': 660,\n",
       " '1800': 661,\n",
       " 'overnight': 662,\n",
       " 'final': 663,\n",
       " 'destination': 664,\n",
       " 'over': 665,\n",
       " 'summer': 666,\n",
       " '297': 667,\n",
       " '1222': 668,\n",
       " '281': 669,\n",
       " 'listed': 670,\n",
       " 'dl': 671,\n",
       " '1055': 672,\n",
       " '405': 673,\n",
       " '201': 674,\n",
       " '315': 675,\n",
       " '21': 676,\n",
       " '486': 677,\n",
       " '825': 678,\n",
       " '555': 679,\n",
       " '1207': 680,\n",
       " '1500': 681,\n",
       " '639': 682,\n",
       " '217': 683,\n",
       " '71': 684,\n",
       " '106': 685,\n",
       " '539': 686,\n",
       " '3724': 687,\n",
       " '271': 688,\n",
       " '1291': 689,\n",
       " '4400': 690,\n",
       " '3357': 691,\n",
       " '345': 692,\n",
       " '771': 693,\n",
       " 'co': 694,\n",
       " '1209': 695,\n",
       " 'ea': 696,\n",
       " '212': 697,\n",
       " '257': 698,\n",
       " '608': 699,\n",
       " '746': 700,\n",
       " 'taking': 701,\n",
       " '311': 702,\n",
       " '417': 703,\n",
       " 'try': 704,\n",
       " 'inform': 705,\n",
       " 'kinds': 706,\n",
       " 'traveling': 707,\n",
       " '419': 708,\n",
       " 'they': 709,\n",
       " 'these': 710,\n",
       " 'kindly': 711,\n",
       " 'proper': 712,\n",
       " 'town': 713,\n",
       " 'takeoff': 714,\n",
       " 'kennedy': 715,\n",
       " 'close': 716,\n",
       " '230': 717,\n",
       " 'nonstops': 718,\n",
       " 'thursdays': 719,\n",
       " \"'re\": 720,\n",
       " '1230': 721,\n",
       " '1200': 722,\n",
       " 'within': 723,\n",
       " 'reservation': 724,\n",
       " 'friends': 725,\n",
       " 'visit': 726,\n",
       " 'here': 727,\n",
       " 'them': 728,\n",
       " 'lives': 729,\n",
       " '0900': 730,\n",
       " '1600': 731,\n",
       " '11': 732,\n",
       " 'we': 733,\n",
       " 'nighttime': 734,\n",
       " 'southwest': 735,\n",
       " 'usa': 736,\n",
       " 'able': 737,\n",
       " 'put': 738,\n",
       " '630': 739,\n",
       " 'nw': 740,\n",
       " 'hp': 741,\n",
       " 'define': 742,\n",
       " 'ff': 743,\n",
       " 'symbols': 744,\n",
       " 'stands': 745,\n",
       " 'kw': 746,\n",
       " 'sam': 747,\n",
       " 'ac': 748,\n",
       " '718': 749,\n",
       " 'wn': 750,\n",
       " 'arrangements': 751,\n",
       " 'sorry': 752,\n",
       " 'must': 753,\n",
       " 'originating': 754,\n",
       " '225': 755,\n",
       " '1158': 756,\n",
       " 'equipment': 757,\n",
       " 'choices': 758,\n",
       " '1205': 759,\n",
       " '1145': 760,\n",
       " 'abbreviations': 761,\n",
       " 'jet': 762,\n",
       " 'companies': 763,\n",
       " 'continuing': 764,\n",
       " 'represented': 765,\n",
       " 'database': 766,\n",
       " 'single': 767,\n",
       " 'rate': 768,\n",
       " 'trips': 769,\n",
       " 'stopovers': 770,\n",
       " 'directly': 771,\n",
       " 'starting': 772,\n",
       " 'afterwards': 773,\n",
       " 'reservations': 774,\n",
       " 'scheduled': 775,\n",
       " 'seat': 776,\n",
       " 'india': 777,\n",
       " 'buy': 778,\n",
       " 'six': 779,\n",
       " '1700': 780,\n",
       " 'say': 781,\n",
       " 'mealtime': 782,\n",
       " '2100': 783,\n",
       " 'economic': 784,\n",
       " 'wish': 785,\n",
       " 'discount': 786,\n",
       " 'staying': 787,\n",
       " 'while': 788,\n",
       " 'look': 789,\n",
       " 'across': 790,\n",
       " 'continent': 791,\n",
       " 'transcontinental': 792,\n",
       " 'begins': 793,\n",
       " 'lands': 794,\n",
       " 'landing': 795,\n",
       " 'month': 796,\n",
       " 'help': 797,\n",
       " '720': 798,\n",
       " '110': 799,\n",
       " 'such': 800,\n",
       " '1045': 801,\n",
       " '934': 802,\n",
       " 'heading': 803,\n",
       " 'toward': 804,\n",
       " '430': 805,\n",
       " 'approximately': 806,\n",
       " '324': 807,\n",
       " '1300': 808,\n",
       " '723': 809,\n",
       " '1020': 810,\n",
       " '645': 811,\n",
       " 'weekday': 812,\n",
       " 'inexpensive': 813,\n",
       " 'thing': 814,\n",
       " 'cheap': 815,\n",
       " 'thanks': 816,\n",
       " 'question': 817,\n",
       " 'live': 818,\n",
       " 'spend': 819,\n",
       " 'seventeen': 820,\n",
       " 'highest': 821,\n",
       " 'priced': 822,\n",
       " 'charges': 823,\n",
       " 'dinnertime': 824,\n",
       " '305': 825,\n",
       " '845': 826,\n",
       " 'noontime': 827,\n",
       " '1026': 828,\n",
       " '823': 829,\n",
       " '2134': 830,\n",
       " '1024': 831,\n",
       " '130': 832,\n",
       " \"'ve\": 833,\n",
       " 'got': 834,\n",
       " 'somebody': 835,\n",
       " 'else': 836,\n",
       " 'wants': 837,\n",
       " '420': 838,\n",
       " 'seven': 839,\n",
       " 'catch': 840,\n",
       " 'fifteen': 841,\n",
       " 'thirteenth': 842,\n",
       " 'tuesdays': 843,\n",
       " 'midway': 844,\n",
       " 'alaska': 845,\n",
       " 'arrange': 846,\n",
       " 'plan': 847,\n",
       " 'oh': 848,\n",
       " 'hello': 849,\n",
       " \"n't\": 850,\n",
       " 'prefer': 851,\n",
       " 'requesting': 852,\n",
       " 'comes': 853,\n",
       " 'mondays': 854,\n",
       " 'bound': 855,\n",
       " 'fridays': 856,\n",
       " 'sundays': 857,\n",
       " 'bay': 858,\n",
       " 'planning': 859,\n",
       " 'home': 860,\n",
       " 'reverse': 861,\n",
       " 'order': 862,\n",
       " 'a.m.': 863,\n",
       " 'philly': 864,\n",
       " 'twelve': 865,\n",
       " 'sd': 866,\n",
       " 'd': 867,\n",
       " '842': 868,\n",
       " '1133': 869,\n",
       " '43': 870,\n",
       " '468': 871,\n",
       " 'al': 872,\n",
       " '950': 873,\n",
       " '55': 874,\n",
       " '279': 875,\n",
       " '137338': 876,\n",
       " 'ap58': 877,\n",
       " '445': 878,\n",
       " '515': 879,\n",
       " 'dtw': 880,\n",
       " 'year': 881,\n",
       " 'phl': 882,\n",
       " 'tpa': 883,\n",
       " \"o'hare\": 884,\n",
       " 'dc9': 885,\n",
       " '1288': 886,\n",
       " 'bn': 887,\n",
       " 'originate': 888,\n",
       " 'concerning': 889,\n",
       " '323': 890,\n",
       " '229': 891,\n",
       " '19': 892,\n",
       " '352': 893,\n",
       " '1083': 894,\n",
       " '98': 895,\n",
       " 'repeating': 896,\n",
       " '650': 897,\n",
       " 'yx': 898,\n",
       " 'gets': 899,\n",
       " '1201': 900,\n",
       " 'just': 901,\n",
       " '819': 902,\n",
       " 'runs': 903,\n",
       " 'everywhere': 904,\n",
       " 'operating': 905,\n",
       " 'currently': 906,\n",
       " 'listings': 907,\n",
       " 'laying': 908,\n",
       " 'sb': 909,\n",
       " '811': 910,\n",
       " '415': 911,\n",
       " 'mia': 912,\n",
       " 'takes': 913,\n",
       " '500': 914,\n",
       " 'airfares': 915,\n",
       " 'wanted': 916,\n",
       " 'c': 917,\n",
       " '2153': 918,\n",
       " '402': 919,\n",
       " 'enroute': 920,\n",
       " '928': 921,\n",
       " 'cp': 922,\n",
       " '810': 923,\n",
       " 'cover': 924,\n",
       " 'provides': 925,\n",
       " 'non': 926,\n",
       " 'once': 927,\n",
       " '615': 928,\n",
       " '815': 929,\n",
       " 'eleven': 930,\n",
       " 'afternoons': 931,\n",
       " 'start': 932,\n",
       " 'place': 933}"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what\n",
      "is\n",
      "the\n",
      "cost\n",
      "of\n",
      "a\n",
      "round\n",
      "what    SYM\n",
      "is    <UNK>\n",
      "the    VERB\n",
      "cost    SYM\n",
      "of    SYM\n",
      "a    CCONJ\n",
      "round    ADJ\n"
     ]
    }
   ],
   "source": [
    "def evaluate(sentence):\n",
    "    sentence = re.sub('[^ A-Za-z0-9]+', '', sentence).split()\n",
    "    tokenized_sent = []\n",
    "    for word in sentence:\n",
    "        if word in word_to_index:\n",
    "            tokenized_sent.append(word_to_index[word])\n",
    "        else:\n",
    "            tokenized_sent.append(word_to_index['<UNK>'])\n",
    "\n",
    "    inputs = torch.tensor(tokenized_sent, dtype=torch.long, device=device)\n",
    "    output = model(inputs)\n",
    "    for i in range(len(sentence)):\n",
    "        print(sentence[i]+\"    \"+index_to_tag[torch.argmax(output[i]).item()])\n",
    "evaluate(\"what is the cost of a round\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
