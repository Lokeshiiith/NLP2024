{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import conllu\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "  device = torch.device(\"cuda\")\n",
    "else:\n",
    "  device = torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = open(\"./conllu/train.conllu\", mode=\"r\", encoding=\"utf-8\")\n",
    "annotated_train_data = train_data.read()\n",
    "sentences = conllu.parse(annotated_train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Got all senteces with padding and each sentence with pos tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = {}\n",
    "tag_to_index = {}\n",
    "max_sentence_length = 0\n",
    "word_count = {}\n",
    "word_to_index['<PAD>'] = 0\n",
    "tag_to_index['<UNK>'] = 0\n",
    "\n",
    "def process_dataset(dataset_file, p=2, s=3):\n",
    "    sentences_list = []\n",
    "    pos_list = []\n",
    "\n",
    "    with open(dataset_file, 'r', encoding='utf-8') as f:\n",
    "        sentence_tokens = []\n",
    "        pos_tags = []\n",
    "\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "\n",
    "            if line.startswith('#'):\n",
    "                sentence_tokens = []\n",
    "                pos_tags = []\n",
    "                continue\n",
    "            elif line == '':\n",
    "                # Append padding to the end of the sentence\n",
    "                padded_sentence = ' '.join(['<PAD>'] * p) + ' ' + ' '.join(sentence_tokens) + ' ' + ' '.join(['<PAD>'] * s)\n",
    "                padded_pos = ' '.join(['<UNK>'] * p + pos_tags + ['<UNK>'] * s)\n",
    "                sentences_list.append(padded_sentence)\n",
    "                pos_list.append(padded_pos)\n",
    "                continue\n",
    "            else:\n",
    "                # New sentence begins\n",
    "                token_attrs = line.split('\\t')\n",
    "                word_form = token_attrs[1]  # Word form of the token\n",
    "                pos_tag = token_attrs[3]    # POS tag of the token\n",
    "                sentence_tokens.append(word_form)\n",
    "                pos_tags.append(pos_tag)\n",
    "\n",
    "    return sentences_list, pos_list\n",
    "\n",
    "# Example usage:\n",
    "dataset_file = './conllu/train.conllu'\n",
    "sentences_list, pos_list = process_dataset(dataset_file, p= 2, s= 3)\n",
    "\n",
    "\n",
    "\n",
    "with open(\"out.txt\", \"w\") as f:\n",
    "    for word in sentences_list:\n",
    "        f.write(f\"{word}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into sentences\n",
    "word_to_index = {'<PAD>': 0, '<UNK>': 1}\n",
    "tag_to_index = {'<UNK>': 0}\n",
    "word_count = {}\n",
    "max_sentence_length = 0\n",
    "\n",
    "# Process each sentence to tokenize the data\n",
    "for sentence_str, tag_str in zip(sentences_list, pos_list):\n",
    "    # Tokenize the sentence into individual tokens\n",
    "    tokens = sentence_str.split(' ')\n",
    "    tags = tag_str.split(' ')\n",
    "        # Word to index\n",
    "    for word, tag in zip(tokens, tags):\n",
    "        if word not in word_to_index:\n",
    "            word_to_index[word] = len(word_to_index)\n",
    "            word_count[word] = word_count.get(word, 0) + 1\n",
    "    # Tag to index\n",
    "        if tag not in tag_to_index:\n",
    "            tag_to_index[tag] = len(tag_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"out.txt\", \"w\") as f:\n",
    "    for word, a in word_to_index.items():\n",
    "        f.write(f\"{word} {a}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Step 1: Define the Model\n",
    "class FFNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, output_size, p, s):\n",
    "        super(FFNN, self).__init__()\n",
    "        # Calculate the actual input size considering embedding dimensions\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.fc1 = nn.Linear((p + s + 1) *embedding_dim , hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten the input tensor\n",
    "        first = self.embedding(x)\n",
    "        first = first.view(-1)\n",
    "        out = self.fc1(first)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['<PAD> <PAD> what is the cost of a round trip flight from pittsburgh to atlanta beginning on april twenty fifth and returning on may sixth <PAD> <PAD> <PAD>'],\n",
       " ['<UNK> <UNK> PRON AUX DET NOUN ADP DET NOUN NOUN NOUN ADP PROPN ADP PROPN VERB ADP NOUN NUM ADJ CCONJ VERB ADP NOUN ADJ <UNK> <UNK> <UNK>'])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen = [sentences_list[0]]\n",
    "pos = [pos_list[0]]\n",
    "sen, pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2)\n",
      "window_tokens_tensor tensor([0, 0, 2, 3, 4, 5])\n",
      "window_pos_tensor tensor([0, 0, 1, 2, 3, 4])\n",
      "predicted tensor(2)\n",
      "outputs tensor([-0.2841, -0.2653,  0.4636, -0.2601,  0.0806, -0.0814,  0.4453, -0.0682,\n",
      "         0.2261,  0.0062,  0.3250, -0.0343, -0.0174,  0.1120],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[130], line 40\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m\"\u001b[39m, outputs)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Calculate loss\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredicted\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_pos_tensor\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Compare outputs with true labels\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n\u001b[0;32m     43\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\Lokes\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Lokes\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1174\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1175\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1176\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Lokes\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\functional.py:3029\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3027\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3028\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3029\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()  # Define the loss function\n",
    "\n",
    "# Step 3: Instantiate Model\n",
    "vocab_size = len(word_to_index)\n",
    "embedding_dim = 100  # Example dimension, adjust as needed\n",
    "hidden_size = 128    # Example size, adjust as needed\n",
    "p = 2\n",
    "s = 3\n",
    "output_size = (s + p + 1)\n",
    "output_size = len(tag_to_index)\n",
    "model = FFNN(vocab_size, embedding_dim, hidden_size, output_size, p, s)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Example optimizer, adjust as needed\n",
    "num_epochs = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Iterate over your dataset\n",
    "    for sentence, pos_tags in zip(sen, pos):  # Consider your data structures here\n",
    "        # Convert tokens and POS tags to indices\n",
    "        token_indices = [word_to_index[token] for token in sentence.strip().split()]\n",
    "        pos_indices = [tag_to_index[pos_tag] for pos_tag in pos_tags.strip().split()]\n",
    "        \n",
    "        # Create sliding window of size 6 and convert to tensors\n",
    "        for i in range(p, len(token_indices) - s):\n",
    "            window_tokens = token_indices[i-p:i+s+1]    \n",
    "            window_tokens_tensor = torch.LongTensor(window_tokens)\n",
    "            window_pos_tensor = torch.LongTensor(pos_indices[i-p:i+s+1])\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(window_tokens_tensor)  # Forward pass\n",
    "            predicted = outputs.argmax() # Get the index of the max logit as the predicted class\n",
    "            print(predicted)\n",
    "            print(\"window_tokens_tensor\", window_tokens_tensor)\n",
    "            print(\"window_pos_tensor\", window_pos_tensor)\n",
    "            print(\"predicted\", predicted)\n",
    "            print(\"outputs\", outputs)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(predicted, window_pos_tensor)  # Compare outputs with true labels\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            print(i, loss.item())  # Print loss for monitoring purposes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 6 is out of bounds for dimension 0 with size 6",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[78], line 39\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# print(outputs, predicted,window_pos_tensor)\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Calculate loss\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(i)\n\u001b[1;32m---> 39\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, \u001b[43mwindow_pos_tensor\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n\u001b[0;32m     42\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "\u001b[1;31mIndexError\u001b[0m: index 6 is out of bounds for dimension 0 with size 6"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 2: Define Loss Function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Step 3: Instantiate Model\n",
    "vocab_size = len(word_to_index)\n",
    "embedding_dim = 100  # Example dimension, adjust as needed\n",
    "hidden_size = 128    # Example size, adjust as needed\n",
    "output_size = len(tag_to_index)\n",
    "p = 2\n",
    "s = 3\n",
    "model = FFNN(vocab_size, embedding_dim, hidden_size, output_size, p, s)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001) # Example optimizer, adjust as needed\n",
    "num_epochs = 10 \n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Iterate over your dataset\n",
    "    for sentence, pos_tags in zip(sentences_list, pos_list):\n",
    "        # Convert tokens and POS tags to indices\n",
    "        token_indices = [word_to_index[token] for token in sentence.strip().split()]\n",
    "        pos_indices = [tag_to_index[pos_tag] for pos_tag in pos_tags.strip().split()]\n",
    "        # Create sliding window of size 6 and convert to tensors\n",
    "        # print(token_indices, pos_indices)\n",
    "        for i in range(p, len(token_indices) - s):\n",
    "            window_tokens = token_indices[i-p:i+s+1]\n",
    "            # window_pos = pos_indices[i+2]  # Assuming you want to predict the POS tag at the center of the window\n",
    "            # Convert to PyTorch tensors\n",
    "            window_tokens_tensor = torch.LongTensor(window_tokens)\n",
    "            window_pos_tensor = torch.LongTensor(pos_indices[i-p:i+s+1])\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(window_tokens_tensor)\n",
    "            predicted = outputs.argmax()\n",
    "            # print(outputs, predicted,window_pos_tensor)\n",
    "\n",
    "            # Calculate loss\n",
    "            print(i)\n",
    "            loss = criterion(outputs, window_pos_tensor[i])\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'encoded_inputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m----> 2\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m \u001b[43mencoded_inputs\u001b[49m:\n\u001b[0;32m      3\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'encoded_inputs' is not defined"
     ]
    }
   ],
   "source": [
    "with open(\"out.txt\", \"w\") as f:\n",
    "    for word in encoded_inputs:\n",
    "        f.write(f\"{word}\\n\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(865, 14, 46)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_to_index), len(tag_to_index), max_sentence_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now iterate over each list of size s + p + 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide data in words (X) and tags (Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorise X and Y\n",
    "Encode X and Y to integer values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./conllu/train.conllu\", \"r\", encoding=\"utf-8\") as f:\n",
    "    train_data = f.read()\n",
    "with open(\"./conllu/val.conllu\", \"r\", encoding=\"utf-8\") as f:\n",
    "    val_data = f.read()\n",
    "with open(\"./conllu/test.conllu\", \"r\", encoding=\"utf-8\") as f:\n",
    "    test_data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into sentences\n",
    "sentences_data = train_data.strip().split(\"\\n\\n\")\n",
    "\n",
    "# Process each sentence to tokenize the data\n",
    "word_to_index = {}\n",
    "tag_to_index = {}\n",
    "max_sentence_length = 0\n",
    "word_count = {}\n",
    "\n",
    "# Initialize special tokens\n",
    "word_to_index['<PAD>'] = 0\n",
    "word_to_index['<UNK>'] = 1\n",
    "tag_to_index['<PAD>'] = 0\n",
    "\n",
    "# Iterate through each sentence\n",
    "for sentence_str in sentences_data:\n",
    "    # Tokenize the sentence into individual tokens\n",
    "    token_list = sentence_str.strip().split(\"\\n\")\n",
    "    for token_str in token_list:\n",
    "        # Ensure the line is not a comment\n",
    "        if token_str[0] != '#':\n",
    "            # Split the token fields\n",
    "            token_fields = token_str.split(\"\\t\")\n",
    "\n",
    "            # Extract token index, word, and tag\n",
    "            token_index = int(token_fields[0])\n",
    "            word = token_fields[1]\n",
    "            tag = token_fields[3]\n",
    "            # ------------------Word to index-------------------\n",
    "            # Add the word to the word_to_index dictionary if it doesn't exist\n",
    "            if word not in word_to_index:\n",
    "                word_to_index[word] = len(word_to_index)\n",
    "                # it will update the size after new word inserted\n",
    "\n",
    "            # Update word count dictionary\n",
    "            if word not in word_count:\n",
    "                word_count[word] = 1\n",
    "            else:\n",
    "                word_count[word] += 1\n",
    "            # ------------------TAg to index-------------------\n",
    "            # Add the tag to the tag_to_index dictionary if it doesn't exist\n",
    "            if tag not in tag_to_index:\n",
    "                tag_to_index[tag] = len(tag_to_index)\n",
    "            # no need to count the tag as it is already in the dictionary\n",
    "            # Update max_sentence_length if needed\n",
    "            max_sentence_length = max(token_index, max_sentence_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def process_data(data, word_to_index, tag_to_index, max_sentence_length, word_count):\n",
    "    input_sequences = []\n",
    "    output_sequences = []\n",
    "    # splitting all sentences\n",
    "    All_sentences_list = data.strip().split(\"\\n\\n\")\n",
    "    for sentence in All_sentences_list:\n",
    "        input_sequence = []\n",
    "        output_sequence = []\n",
    "\n",
    "        tokens_strings = sentence.strip().split(\"\\n\")\n",
    "        for each_token in tokens_strings:\n",
    "            if each_token[0] == '#':\n",
    "                continue\n",
    "            else:\n",
    "                # as differnt fields\n",
    "                # ex - 5\tof\tof\tADP\t_\t_\t7\tcase\n",
    "                fields = each_token.split(\"\\t\")\n",
    "                word = fields[1]\n",
    "                tag = fields[3]\n",
    "\n",
    "                if word in word_to_index:\n",
    "                    if word_count[word] < 2:\n",
    "                        word_cur_idx = word_to_index['<UNK>']\n",
    "                    else:\n",
    "                        word_cur_idx = word_to_index[word]\n",
    "                else:\n",
    "                    word_cur_idx = word_to_index['<UNK>']\n",
    "                \n",
    "                if tag in tag_to_index:\n",
    "                    tag_cur_idx = tag_to_index[tag]\n",
    "                else:\n",
    "                    # what to do here\n",
    "                    tag_cur_idx = tag_to_index['PRON']\n",
    "                #For each word append current word index to input_sequence and tag index to output_sequence \n",
    "                input_sequence.append(word_cur_idx)\n",
    "                output_sequence.append(tag_cur_idx)\n",
    "        \n",
    "        # Pad sequences using PyTorch's pad_sequence function\n",
    "        input_sequence = torch.tensor(input_sequence)\n",
    "        output_sequence = torch.tensor(output_sequence)\n",
    "        input_sequence = torch.nn.functional.pad(input_sequence, (0, max_sentence_length - len(input_sequence)))\n",
    "        output_sequence = torch.nn.functional.pad(output_sequence, (0, max_sentence_length - len(output_sequence)))\n",
    "        input_sequences.append(input_sequence)\n",
    "        output_sequences.append(output_sequence)\n",
    "     # Stack sequences to create tensors\n",
    "    input_sequences = torch.stack(input_sequences).numpy()\n",
    "    output_sequences = torch.stack(output_sequences).numpy()\n",
    "\n",
    "    return input_sequences, output_sequences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = process_data(train_data, word_to_index, tag_to_index, max_sentence_length, word_count)\n",
    "X_val, y_val = process_data(val_data, word_to_index, tag_to_index, max_sentence_length, word_count)\n",
    "X_test, y_test = process_data(test_data, word_to_index, tag_to_index, max_sentence_length, word_count)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# Assuming X_train is your input data\n",
    "scaler = StandardScaler()\n",
    "X_train_normalized = scaler.fit_transform(X_train)\n",
    "y_train_normalized = scaler.fit_transform(y_train)\n",
    "X_val_normalized = scaler.fit_transform(X_val)\n",
    "y_val_normalized = scaler.fit_transform(y_val)\n",
    "X_test_normalized = scaler.fit_transform(X_test)\n",
    "y_test_normalized = scaler.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"out.txt\", \"w\") as f:\n",
    "    # save input sequences to file\n",
    "    for i in y_test_normalized:\n",
    "        f.write(f\"{i}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Create PyTorch DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create PyTorch datasets and data loaders for training data. This allows efficient batching and shuffling of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_train.shape[1] is used to specify the input size of the neural network model.     X_train.shape[1] accesses the second element of the shape tuple, which represents the number of features or input dimensions in your dataset. In the example (1000, 50), X_train.shape[1] would be 50.\n",
    "\n",
    "Sending X_train.shape[1] as the input_size parameter to the FFNN constructor ensures that the input layer of your neural network has the correct number of neurons to accommodate the input features of your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Define Loss Function and Optimizer\n",
    "model = FFNN(input_size=X_train.shape[1], hidden_size=128, output_size=y_train.shape[1])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Step 4: Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 1942.5025033523787\n",
      "Epoch [2/20], Loss: 1406.451219245569\n",
      "Epoch [3/20], Loss: 1080.4993978471898\n",
      "Epoch [4/20], Loss: 874.9364979302705\n",
      "Epoch [5/20], Loss: 744.392890588561\n",
      "Epoch [6/20], Loss: 654.735463156629\n",
      "Epoch [7/20], Loss: 589.1151683294951\n",
      "Epoch [8/20], Loss: 538.1906392111707\n",
      "Epoch [9/20], Loss: 498.44278773976794\n",
      "Epoch [10/20], Loss: 465.6859440590019\n",
      "Epoch [11/20], Loss: 438.57284591447063\n",
      "Epoch [12/20], Loss: 415.38864682325675\n",
      "Epoch [13/20], Loss: 395.12138571668027\n",
      "Epoch [14/20], Loss: 377.8649383089436\n",
      "Epoch [15/20], Loss: 362.8608712723006\n",
      "Epoch [16/20], Loss: 348.303059819919\n",
      "Epoch [17/20], Loss: 335.64912983908584\n",
      "Epoch [18/20], Loss: 324.2286862045971\n",
      "Epoch [19/20], Loss: 313.86049435743644\n",
      "Epoch [20/20], Loss: 303.9825999701201\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "# for training \n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs.float())\n",
    "        loss = criterion(outputs, labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1000):  # Adjust number of epochs as needed\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_tag = {v: k for k, v in tag_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 7 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[105], line 33\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     32\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# Set model to evaluation mode\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_sequence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Map output predictions to POS tags\u001b[39;00m\n\u001b[0;32m     36\u001b[0m predicted_tags \u001b[38;5;241m=\u001b[39m [index_to_tag[torch\u001b[38;5;241m.\u001b[39margmax(output)\u001b[38;5;241m.\u001b[39mitem()] \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs]\n",
      "File \u001b[1;32mc:\\Users\\Lokes\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[71], line 11\u001b[0m, in \u001b[0;36mFFNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# Ensure input tensor has the same data type as the weight matrices\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m     12\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x)\n\u001b[0;32m     13\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "# Example input sentence\n",
    "input_sentence = \"Mary had a little lamb\"\n",
    "# Tokenize the sentence and convert it to a sequence of word indices\n",
    "words = sentence.split()\n",
    "input_sequence = []\n",
    "for word in words:\n",
    "    if word in word_to_index:\n",
    "        if word_count[word] < 2:\n",
    "            input_sequence.append(word_to_index['<UNK>'])\n",
    "        else:\n",
    "            input_sequence.append(word_to_index[word])\n",
    "    else:\n",
    "        input_sequence.append(word_to_index['<UNK>'])\n",
    "\n",
    "# Tokenize the input sentence\n",
    "# input_sequence = input_sentence.split()\n",
    "# Pad the sequence with zeros to make it the same length as max_sen_len\n",
    "if len(input_sequence) < max_sentence_length:\n",
    "    input_sequence += [0] * (max_sentence_length - len(input_sequence))\n",
    "\n",
    "# Convert tokens to indices using the vocabulary\n",
    "input_indices = [word_to_index.get(token, word_to_index['<UNK>']) for token in input_sequence]\n",
    "# Pad the input sequence if necessary\n",
    "input_sequence = np.array(input_sequence).reshape(1, max_sentence_length)\n",
    "print(input_sequence)\n",
    "# Convert input indices to PyTorch tensor\n",
    "# input_tensor = torch.tensor(input_sequence)\n",
    "# Ensure input tensor has the correct shape and type if needed\n",
    "\n",
    "# Pass the input through the model\n",
    "with torch.no_grad():\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    outputs = model(input_sequence)\n",
    "\n",
    "# Map output predictions to POS tags\n",
    "predicted_tags = [index_to_tag[torch.argmax(output).item()] for output in outputs]\n",
    "\n",
    "print(\"Input sentence:\", input_sentence)\n",
    "print(\"Predicted tags:\", predicted_tags)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question2 : 2 Recurrent Neural Network POS Tagging\n",
    "Design and implement a model which uses Recurrent Neural Networks (Vanilla\n",
    "RNN, LSTM, or GRU) for POS Tagging. The model should take the embed\u0002dings for all tokens in a sentence and output the corresponding POS tags in\n",
    "sequence.\n",
    "- For Example: In the sentence \"An apple a day keeps the doctor away\",\n",
    " the model takes the embeddings for \n",
    "- [\"An\", \"apple\", \"a\", \"day\", \"keeps\", \"the\",\"doctor\", \"away\"] and\n",
    " outputs the POS tags for all the words in the sentence\n",
    "- [\"DET\", \"NOUN\", \"DET\", \"NOUN\", \"VERB\", \"DET\", \"NOUN\", \"ADV\"] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step1 : Count all words and postags and provide them a index value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Doing it for all 3 datasets train, validation and test-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./conllu/train.conllu\", \"r\", encoding=\"utf-8\") as f:\n",
    "    train_data = f.read()\n",
    "with open(\"./conllu/val.conllu\", \"r\", encoding=\"utf-8\") as f:\n",
    "    val_data = f.read()\n",
    "with open(\"./conllu/test.conllu\", \"r\", encoding=\"utf-8\") as f:\n",
    "    test_data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(dataset_file, p=2, s=3):\n",
    "    sentences_list = []\n",
    "    pos_list = []\n",
    "\n",
    "    with open(dataset_file, 'r') as f:\n",
    "        sentence_tokens = []\n",
    "        pos_tags = []\n",
    "\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "\n",
    "            if line.startswith('#'):\n",
    "                sentence_tokens = []\n",
    "                pos_tags = []\n",
    "                continue\n",
    "            elif line == '':\n",
    "                # Append padding to the end of the sentence\n",
    "                padded_sentence = ' '.join(['<PAD>'] * p) + ' ' + ' '.join(sentence_tokens) + ' ' + ' '.join(['<PAD>'] * s)\n",
    "                padded_pos = ' '.join(['<UNK>'] * p + pos_tags + ['<UNK>'] * s)\n",
    "                sentences_list.append(padded_sentence)\n",
    "                pos_list.append(padded_pos)\n",
    "                continue\n",
    "            else:\n",
    "                # New sentence begins\n",
    "                token_attrs = line.split('\\t')\n",
    "                word_form = token_attrs[1]  # Word form of the token\n",
    "                pos_tag = token_attrs[3]    # POS tag of the token\n",
    "                sentence_tokens.append(word_form)\n",
    "                pos_tags.append(pos_tag)\n",
    "\n",
    "    return sentences_list, pos_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for all train, test and validation dataset\n",
    "- I got all sentences tokens and respective pos-tag in form of sentence sepateted by space \n",
    "- Max sentece length, word count , word to index adn tag t index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into sentences\n",
    "word_to_index = {'<PAD>': 0, '<UNK>': 1}\n",
    "tag_to_index = {'<UNK>': 0}\n",
    "word_count = {}\n",
    "max_sentence_length = 0\n",
    "train_dataset = \"./conllu/train.conllu\"\n",
    "test_dataset = \"./conllu/test.conllu\"\n",
    "val_dataset = \"./conllu/val.conllu\"\n",
    "train_sentece_list, train_pos_list = process_dataset(train_dataset, p= 2, s= 3)\n",
    "test_sentece_list, test_pos_list = process_dataset(test_dataset, p= 2, s= 3)\n",
    "val_sentece_list, val_pos_list = process_dataset(val_dataset, p= 2, s= 3)\n",
    "\n",
    "sentences_list = train_sentece_list + test_sentece_list + val_sentece_list\n",
    "pos_list = train_pos_list + test_pos_list + val_pos_list\n",
    "\n",
    "# Process each sentence to tokenize the data\n",
    "for sentence_str, tag_str in zip(sentences_list, pos_list):\n",
    "    # Tokenize the sentence into individual tokens\n",
    "    tokens = sentence_str.split(' ')\n",
    "    tags = tag_str.split(' ')\n",
    "        # Word to index\n",
    "    for word, tag in zip(tokens, tags):\n",
    "        if word not in word_to_index:\n",
    "            word_to_index[word] = len(word_to_index)\n",
    "            word_count[word] = word_count.get(word, 0) + 1\n",
    "    # Tag to index\n",
    "        if tag not in tag_to_index:\n",
    "            tag_to_index[tag] = len(tag_to_index)\n",
    "    max_sentence_length = max(max_sentence_length, len(tokens))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"out.txt\", \"w\") as f:\n",
    "    for word in pos_list:\n",
    "        f.write(f\"{word}\\n\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PAdding and provide tag <unk> to those words wshich comes once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- train_sentece_list, train_pos_list  \n",
    "- test_sentece_list, test_pos_list \n",
    "- val_sentece_list, val_pos_list  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"out.txt\", \"w\") as f:\n",
    "    for word, a in word_to_index.items():\n",
    "        f.write(f\"{word} {a}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
