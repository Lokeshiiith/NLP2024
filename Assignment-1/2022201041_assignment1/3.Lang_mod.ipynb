{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Smoothing and Interpolation\n",
    "You have been given two corpora: “Pride and Prejudice” and “Ulysses”. Your\n",
    "task is to design Language Models for both using smoothing techniques. Be\n",
    "sure to use the tokenizer created in the Tokenization task.\n",
    "1. Create language models with the following parameters:\n",
    "- (a).  On “Pride and Prejudice” corpus:\n",
    "- i. LM 1: Tokenization + 3-gram LM + Good-Turing Smoothing\n",
    "- ii. LM 2: Tokenization + 3-gram LM + Linear Interpolation\n",
    "- (b) On “Ulysses” corpus:\n",
    "- i. LM 3: Tokenization + 3-gram LM + Good-Turing Smoothing\n",
    "- ii. LM 4: Tokenization + 3-gram LM + Linear Interpolation\n",
    "2. For each of these corpora, create a test set by randomly selecting 1000\n",
    "sentences. This set will not be used for training the LM.\n",
    "- (a) Calculate perplexity score for each sentence of “Pride and Prejudice”\n",
    "corpus and “Ulysses” corpus for each of the above models\n",
    "and also get average perplexity score on the train corpus.\n",
    "- (b) Report the perplexity scores for all the sentences in the training\n",
    "set. Report the perplexity score on the test sentences as well, in\n",
    "the same manner above\n",
    "- Note: Good-Turing Smoothing is a technique used to adjust the probability\n",
    "distribution of unseen events in N-gram models, \n",
    "- while Linear - Interpolation\n",
    "is a method of combining different N-gram models (like unigram,\n",
    "bigram, trigram) to get a better estimate of the probabilities. For more\n",
    "details, refer to the resources section at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Focus on good turing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "def test_train_split(corpus, n):\n",
    "    # remove new line\n",
    "    corpus = corpus.replace('\\n', ' ')\n",
    "    # split into sentences\n",
    "    sentences = re.split(r'(?<=[.!?]) +', corpus)\n",
    "    test_sentences = random.sample(sentences, n)\n",
    "    train_sentences = [sentence for sentence in sentences if sentence not in test_sentences]\n",
    "    return test_sentences, train_sentences\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    url_pattern1 = \"(http|ftp|https):\\/\\/([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:\\/~+#-]*[\\w@?^=%&\\/~+#-])\"\n",
    "    url_pattern2 = r'www\\.[^\\s\\.]+(?:\\.[^\\s\\.]+)*(?:[\\s\\.]|$)'\n",
    "    email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
    "    mention_pattern = \"@\\w+\"\n",
    "    hastag_pattern = \"#[a-z0-9_]+\"\n",
    "    normal_pattern = \"[a-zA-Z]+\"\n",
    "    number_pattern = \"[0-9]+\"\n",
    "    pattern = r'[,\"\\'()\\[\\]{}-]'  # Matches , \" ' ( ) [ ] { }\n",
    "    tokens = []\n",
    "    text = text.lower()\n",
    "    text = re.sub(pattern, '', text)\n",
    "    text = re.sub(url_pattern1, '<URL> ', text)\n",
    "    text = re.sub(url_pattern2, '<URL> ', text)\n",
    "    text = re.sub(email_pattern, '<MAILID> ', text)\n",
    "    text = re.sub(hastag_pattern, '<HASHTAG> ', text)\n",
    "    text = re.sub(mention_pattern, '<MENTION> ', text)\n",
    "    text = re.sub(number_pattern, '<NUM> ', text)\n",
    "    tokens = re.findall(r'\\b\\w+|[^\\s\\w<>]+|<\\w+>', text)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "coupus_path = './corpus'\n",
    "corpus1 = \"./corpus/Pride and Prejudice - Jane Austen.txt\"\n",
    "corpus2 = \"./corpus/Ulysses  James Joyce.txt\"\n",
    "with open(corpus1, 'r') as f:\n",
    "    text1 = f.read()\n",
    "test_sentences, train_sentences = test_train_split(text1, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_cleaning(text):\n",
    "    # remove comma, extra spaces, and punctuations\n",
    "    text = re.sub(r'[,!?;-]+', '', text)\n",
    "    if text.endswith('.'):\n",
    "            text = text[:-1]#removing last dot also\n",
    "    return text\n",
    "def PerformNgram(corpus, n):\n",
    "    pattern = \"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s\"\n",
    "    list_sentences = re.split(pattern, corpus)\n",
    "    ngrams = {}\n",
    "    for sentence in list_sentences:\n",
    "        tokens = tokenize(sentence)\n",
    "        # sentence = (n-1)*\"<START> \"+ sentence\n",
    "        for i in range(len(tokens)-n+1):\n",
    "            temp = tuple(tokens[j] for j in range(i, i+n))  # Convert list to tuple\n",
    "            if temp in ngrams:\n",
    "                ngrams[temp] += 1\n",
    "            else:\n",
    "                ngrams[temp] = 1\n",
    "            \n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('the', 'project', 'gutenberg'): 16,\n",
       " ('project', 'gutenberg', 'ebook'): 3,\n",
       " ('gutenberg', 'ebook', 'pride'): 3,\n",
       " ('ebook', 'pride', 'and'): 3,\n",
       " ('pride', 'and', 'prejudice'): 6,\n",
       " ('and', 'prejudice', 'by'): 1,\n",
       " ('prejudice', 'by', 'jane'): 1,\n",
       " ('by', 'jane', 'austen'): 1,\n",
       " ('jane', 'austen', 'edited'): 1,\n",
       " ('austen', 'edited', 'by'): 1,\n",
       " ('edited', 'by', 'r'): 1,\n",
       " ('by', 'r', '.'): 1,\n",
       " ('robert', 'william', 'chapman'): 2,\n",
       " ('william', 'chapman', 'this'): 1,\n",
       " ('chapman', 'this', 'ebook'): 1,\n",
       " ('this', 'ebook', 'is'): 1,\n",
       " ('ebook', 'is', 'for'): 1,\n",
       " ('is', 'for', 'the'): 1,\n",
       " ('for', 'the', 'use'): 2,\n",
       " ('the', 'use', 'of'): 3,\n",
       " ('use', 'of', 'anyone'): 1,\n",
       " ('of', 'anyone', 'anywhere'): 1,\n",
       " ('anyone', 'anywhere', 'at'): 1,\n",
       " ('anywhere', 'at', 'no'): 1,\n",
       " ('at', 'no', 'cost'): 1,\n",
       " ('no', 'cost', 'and'): 1,\n",
       " ('cost', 'and', 'with'): 1,\n",
       " ('and', 'with', 'almost'): 1,\n",
       " ('with', 'almost', 'no'): 1,\n",
       " ('almost', 'no', 'restrictions'): 1,\n",
       " ('no', 'restrictions', 'whatsoever'): 1,\n",
       " ('restrictions', 'whatsoever', '.'): 1,\n",
       " ('you', 'may', 'copy'): 1,\n",
       " ('may', 'copy', 'it'): 1,\n",
       " ('copy', 'it', 'give'): 1,\n",
       " ('it', 'give', 'it'): 1,\n",
       " ('give', 'it', 'away'): 1,\n",
       " ('it', 'away', 'or'): 1,\n",
       " ('away', 'or', 'reuse'): 1,\n",
       " ('or', 'reuse', 'it'): 1,\n",
       " ('reuse', 'it', 'under'): 1,\n",
       " ('it', 'under', 'the'): 1,\n",
       " ('under', 'the', 'terms'): 1,\n",
       " ('the', 'terms', 'of'): 10,\n",
       " ('terms', 'of', 'the'): 6,\n",
       " ('of', 'the', 'project'): 8,\n",
       " ('project', 'gutenberg', 'license'): 2,\n",
       " ('gutenberg', 'license', 'included'): 1,\n",
       " ('license', 'included', 'with'): 1,\n",
       " ('included', 'with', 'this'): 1,\n",
       " ('with', 'this', 'ebook'): 1,\n",
       " ('this', 'ebook', 'or'): 1,\n",
       " ('ebook', 'or', 'online'): 1,\n",
       " ('or', 'online', 'at'): 2,\n",
       " ('online', 'at', '<URL>'): 2,\n",
       " ('at', '<URL>', 'title'): 1,\n",
       " ('<URL>', 'title', ':'): 1,\n",
       " ('title', ':', 'pride'): 1,\n",
       " (':', 'pride', 'and'): 1,\n",
       " ('and', 'prejudice', 'author'): 1,\n",
       " ('prejudice', 'author', ':'): 1,\n",
       " ('author', ':', 'jane'): 1,\n",
       " (':', 'jane', 'austen'): 1,\n",
       " ('jane', 'austen', 'editor'): 1,\n",
       " ('austen', 'editor', ':'): 1,\n",
       " ('editor', ':', 'r'): 1,\n",
       " (':', 'r', '.'): 1,\n",
       " ('william', 'chapman', 'release'): 1,\n",
       " ('chapman', 'release', 'date'): 1,\n",
       " ('release', 'date', ':'): 1,\n",
       " ('date', ':', 'may'): 1,\n",
       " (':', 'may', '<NUM>'): 1,\n",
       " ('may', '<NUM>', '<NUM>'): 1,\n",
       " ('<NUM>', '<NUM>', 'ebook'): 1,\n",
       " ('<NUM>', 'ebook', '<HASHTAG>'): 1,\n",
       " ('ebook', '<HASHTAG>', 'language'): 1,\n",
       " ('<HASHTAG>', 'language', ':'): 1,\n",
       " ('language', ':', 'english'): 1,\n",
       " (':', 'english', '***'): 1,\n",
       " ('english', '***', 'start'): 1,\n",
       " ('***', 'start', 'of'): 1,\n",
       " ('start', 'of', 'the'): 1,\n",
       " ('and', 'prejudice', '***'): 2,\n",
       " ('prejudice', '***', 'etext'): 1,\n",
       " ('***', 'etext', 'prepared'): 1,\n",
       " ('etext', 'prepared', 'by'): 1,\n",
       " ('prepared', 'by', 'greg'): 1,\n",
       " ('by', 'greg', 'weeks'): 1,\n",
       " ('greg', 'weeks', 'jon'): 1,\n",
       " ('weeks', 'jon', 'hurst'): 1,\n",
       " ('jon', 'hurst', 'mary'): 1,\n",
       " ('hurst', 'mary', 'meehan'): 1,\n",
       " ('mary', 'meehan', 'and'): 1,\n",
       " ('meehan', 'and', 'the'): 1,\n",
       " ('and', 'the', 'online'): 1,\n",
       " ('the', 'online', 'distributed'): 1,\n",
       " ('online', 'distributed', 'proofreading'): 1,\n",
       " ('distributed', 'proofreading', 'team'): 1,\n",
       " ('proofreading', 'team', '<URL>'): 1,\n",
       " ('team', '<URL>', 'from'): 1,\n",
       " ('<URL>', 'from', 'page'): 1,\n",
       " ('from', 'page', 'images'): 1,\n",
       " ('page', 'images', 'generously'): 1,\n",
       " ('images', 'generously', 'made'): 1,\n",
       " ('generously', 'made', 'available'): 1,\n",
       " ('made', 'available', 'by'): 1,\n",
       " ('available', 'by', 'internet'): 1,\n",
       " ('by', 'internet', 'archive'): 1,\n",
       " ('internet', 'archive', '<URL>'): 1,\n",
       " ('archive', '<URL>', 'note'): 1,\n",
       " ('<URL>', 'note', ':'): 1,\n",
       " ('note', ':', 'project'): 1,\n",
       " (':', 'project', 'gutenberg'): 1,\n",
       " ('project', 'gutenberg', 'also'): 1,\n",
       " ('gutenberg', 'also', 'has'): 1,\n",
       " ('also', 'has', 'an'): 1,\n",
       " ('has', 'an', 'html'): 1,\n",
       " ('an', 'html', 'version'): 1,\n",
       " ('html', 'version', 'of'): 1,\n",
       " ('version', 'of', 'this'): 1,\n",
       " ('of', 'this', 'file'): 1,\n",
       " ('this', 'file', 'which'): 1,\n",
       " ('file', 'which', 'includes'): 1,\n",
       " ('which', 'includes', 'the'): 1,\n",
       " ('includes', 'the', 'original'): 1,\n",
       " ('the', 'original', 'illustrations'): 1,\n",
       " ('original', 'illustrations', '.'): 1,\n",
       " ('see', '<NUM>', 'h'): 1,\n",
       " ('<NUM>', 'h', '.'): 2,\n",
       " ('h', '.', 'htm'): 1,\n",
       " ('.', 'htm', 'or'): 1,\n",
       " ('htm', 'or', '<NUM>'): 1,\n",
       " ('or', '<NUM>', 'h'): 1,\n",
       " ('h', '.', 'zip'): 1,\n",
       " ('.', 'zip', ':'): 1,\n",
       " ('zip', ':', '<URL>'): 1,\n",
       " (':', '<URL>', 'or'): 1,\n",
       " ('<URL>', 'or', '<URL>'): 1,\n",
       " ('or', '<URL>', 'images'): 1,\n",
       " ('<URL>', 'images', 'of'): 1,\n",
       " ('images', 'of', 'the'): 1,\n",
       " ('of', 'the', 'original'): 1,\n",
       " ('the', 'original', 'pages'): 1,\n",
       " ('original', 'pages', 'are'): 1,\n",
       " ('pages', 'are', 'available'): 1,\n",
       " ('are', 'available', 'through'): 1,\n",
       " ('available', 'through', 'internet'): 1,\n",
       " ('through', 'internet', 'archive'): 1,\n",
       " ('internet', 'archive', '.'): 1,\n",
       " ('see', '<URL>', 'transcribers'): 1,\n",
       " ('<URL>', 'transcribers', 'note'): 1,\n",
       " ('transcribers', 'note', ':'): 2,\n",
       " ('note', ':', 'text'): 1,\n",
       " (':', 'text', 'enclosed'): 1,\n",
       " ('text', 'enclosed', 'by'): 1,\n",
       " ('enclosed', 'by', 'underscores'): 1,\n",
       " ('by', 'underscores', 'is'): 1,\n",
       " ('underscores', 'is', 'in'): 1,\n",
       " ('is', 'in', 'italics'): 1,\n",
       " ('in', 'italics', '_italics_'): 1,\n",
       " ('italics', '_italics_', '.'): 1,\n",
       " ('a', 'carat', 'character'): 1,\n",
       " ('carat', 'character', 'is'): 1,\n",
       " ('character', 'is', 'used'): 1,\n",
       " ('is', 'used', 'to'): 1,\n",
       " ('used', 'to', 'denote'): 1,\n",
       " ('to', 'denote', 'superscription'): 1,\n",
       " ('denote', 'superscription', '.'): 1,\n",
       " ('multiple', 'superscripted', 'characters'): 1,\n",
       " ('superscripted', 'characters', 'are'): 1,\n",
       " ('characters', 'are', 'enclosed'): 1,\n",
       " ('are', 'enclosed', 'by'): 1,\n",
       " ('enclosed', 'by', 'curly'): 1,\n",
       " ('by', 'curly', 'brackets'): 1,\n",
       " ('curly', 'brackets', 'example'): 1,\n",
       " ('brackets', 'example', ':'): 1,\n",
       " ('example', ':', 'm'): 1,\n",
       " (':', 'm', '^'): 1,\n",
       " ('m', '^', 'rs'): 2,\n",
       " ('^', 'rs', '.'): 1,\n",
       " ('in', 'three', 'volumes'): 3,\n",
       " ('three', 'volumes', '.'): 3,\n",
       " ('by', 'the', 'author'): 3,\n",
       " ('the', 'author', 'of'): 3,\n",
       " ('author', 'of', 'sense'): 3,\n",
       " ('of', 'sense', 'and'): 4,\n",
       " ('sense', 'and', 'sensibility'): 3,\n",
       " ('and', 'sensibility', '.'): 3,\n",
       " ('sensibility', '.', 'vol'): 3,\n",
       " ('.', 'vol', '.'): 3,\n",
       " ('egerton', 'military', 'library'): 3,\n",
       " ('military', 'library', 'whitehall'): 3,\n",
       " ('library', 'whitehall', '.'): 3,\n",
       " ('illustration', ':', 'morning'): 1,\n",
       " (':', 'morning', 'dress'): 1,\n",
       " ('morning', 'dress', '.'): 1,\n",
       " ('_invented', 'by', 'm'): 1,\n",
       " ('by', 'm', '^'): 1,\n",
       " ('^', 'rs', 'bell'): 1,\n",
       " ('rs', 'bell', '<NUM>'): 1,\n",
       " ('bell', '<NUM>', 'charlotte'): 1,\n",
       " ('<NUM>', 'charlotte', 'street'): 1,\n",
       " ('charlotte', 'street', 'bedford'): 1,\n",
       " ('street', 'bedford', 'square'): 1,\n",
       " ('bedford', 'square', '.'): 1,\n",
       " ('square', '.', '_'): 1,\n",
       " ('.', '_', '_engraved'): 1,\n",
       " ('_', '_engraved', 'for'): 1,\n",
       " ('_engraved', 'for', 'no'): 1,\n",
       " ('for', 'no', '.'): 1,\n",
       " ('no', '.', '<NUM>'): 1,\n",
       " ('.', '<NUM>', 'of'): 1,\n",
       " ('<NUM>', 'of', 'la'): 1,\n",
       " ('of', 'la', 'belle'): 1,\n",
       " ('la', 'belle', 'assemblee'): 1,\n",
       " ('belle', 'assemblee', '<NUM>'): 1,\n",
       " ('assemblee', '<NUM>', '^'): 1,\n",
       " ('<NUM>', '^', 'st'): 1,\n",
       " ('^', 'st', 'july'): 1,\n",
       " ('st', 'july', '<NUM>'): 1,\n",
       " ('july', '<NUM>', '_'): 1,\n",
       " ('<NUM>', '_', 'pride'): 1,\n",
       " ('_', 'pride', '&'): 1,\n",
       " ('pride', '&', 'prejudice'): 3,\n",
       " ('&', 'prejudice', '.'): 3,\n",
       " ('chapter', 'i', '.'): 3,\n",
       " ('it', 'is', 'a'): 17,\n",
       " ('is', 'a', 'truth'): 1,\n",
       " ('a', 'truth', 'universally'): 1,\n",
       " ('truth', 'universally', 'acknowledged'): 1,\n",
       " ('universally', 'acknowledged', 'that'): 1,\n",
       " ('acknowledged', 'that', 'a'): 1,\n",
       " ('that', 'a', 'single'): 1,\n",
       " ('a', 'single', 'man'): 2,\n",
       " ('single', 'man', 'in'): 1,\n",
       " ('man', 'in', 'possession'): 1,\n",
       " ('in', 'possession', 'of'): 2,\n",
       " ('possession', 'of', 'a'): 1,\n",
       " ('of', 'a', 'good'): 1,\n",
       " ('a', 'good', 'fortune'): 1,\n",
       " ('good', 'fortune', 'must'): 1,\n",
       " ('fortune', 'must', 'be'): 1,\n",
       " ('must', 'be', 'in'): 4,\n",
       " ('be', 'in', 'want'): 1,\n",
       " ('in', 'want', 'of'): 2,\n",
       " ('want', 'of', 'a'): 2,\n",
       " ('of', 'a', 'wife'): 2,\n",
       " ('a', 'wife', '.'): 2,\n",
       " ('however', 'little', 'known'): 1,\n",
       " ('little', 'known', 'the'): 1,\n",
       " ('known', 'the', 'feelings'): 1,\n",
       " ('the', 'feelings', 'or'): 1,\n",
       " ('feelings', 'or', 'views'): 1,\n",
       " ('or', 'views', 'of'): 1,\n",
       " ('views', 'of', 'such'): 1,\n",
       " ('of', 'such', 'a'): 9,\n",
       " ('such', 'a', 'man'): 4,\n",
       " ('a', 'man', 'may'): 1,\n",
       " ('man', 'may', 'be'): 1,\n",
       " ('may', 'be', 'on'): 1,\n",
       " ('be', 'on', 'his'): 1,\n",
       " ('on', 'his', 'first'): 3,\n",
       " ('his', 'first', 'entering'): 1,\n",
       " ('first', 'entering', 'a'): 1,\n",
       " ('entering', 'a', 'neighbourhood'): 1,\n",
       " ('a', 'neighbourhood', 'this'): 1,\n",
       " ('neighbourhood', 'this', 'truth'): 1,\n",
       " ('this', 'truth', 'is'): 1,\n",
       " ('truth', 'is', 'so'): 1,\n",
       " ('is', 'so', 'well'): 1,\n",
       " ('so', 'well', 'fixed'): 1,\n",
       " ('well', 'fixed', 'in'): 1,\n",
       " ('fixed', 'in', 'the'): 1,\n",
       " ('in', 'the', 'minds'): 1,\n",
       " ('the', 'minds', 'of'): 1,\n",
       " ('minds', 'of', 'the'): 1,\n",
       " ('of', 'the', 'surrounding'): 1,\n",
       " ('the', 'surrounding', 'families'): 1,\n",
       " ('surrounding', 'families', 'that'): 1,\n",
       " ('families', 'that', 'he'): 1,\n",
       " ('that', 'he', 'is'): 10,\n",
       " ('he', 'is', 'considered'): 1,\n",
       " ('is', 'considered', 'as'): 1,\n",
       " ('considered', 'as', 'the'): 1,\n",
       " ('as', 'the', 'rightful'): 1,\n",
       " ('the', 'rightful', 'property'): 1,\n",
       " ('rightful', 'property', 'of'): 1,\n",
       " ('property', 'of', 'some'): 1,\n",
       " ('of', 'some', 'one'): 2,\n",
       " ('some', 'one', 'or'): 1,\n",
       " ('one', 'or', 'other'): 1,\n",
       " ('or', 'other', 'of'): 2,\n",
       " ('other', 'of', 'their'): 1,\n",
       " ('of', 'their', 'daughters'): 1,\n",
       " ('their', 'daughters', '.'): 1,\n",
       " ('my', 'dear', 'mr'): 5,\n",
       " ('dear', 'mr', '.'): 5,\n",
       " ('mr', '.', 'bennet'): 51,\n",
       " ('.', 'bennet', 'said'): 2,\n",
       " ('bennet', 'said', 'his'): 1,\n",
       " ('said', 'his', 'lady'): 1,\n",
       " ('his', 'lady', 'to'): 1,\n",
       " ('lady', 'to', 'him'): 1,\n",
       " ('to', 'him', 'one'): 1,\n",
       " ('him', 'one', 'day'): 1,\n",
       " ('one', 'day', 'have'): 1,\n",
       " ('day', 'have', 'you'): 1,\n",
       " ('have', 'you', 'heard'): 4,\n",
       " ('you', 'heard', 'that'): 1,\n",
       " ('heard', 'that', 'netherfield'): 1,\n",
       " ('that', 'netherfield', 'park'): 1,\n",
       " ('netherfield', 'park', 'is'): 1,\n",
       " ('park', 'is', 'let'): 1,\n",
       " ('is', 'let', 'at'): 1,\n",
       " ('let', 'at', 'last'): 1,\n",
       " ('at', 'last', '?'): 1,\n",
       " ('last', '?', 'mr'): 1,\n",
       " ('?', 'mr', '.'): 7,\n",
       " ('.', 'bennet', 'replied'): 2,\n",
       " ('bennet', 'replied', 'that'): 1,\n",
       " ('replied', 'that', 'he'): 4,\n",
       " ('that', 'he', 'had'): 28,\n",
       " ('he', 'had', 'not'): 9,\n",
       " ('had', 'not', '.'): 1,\n",
       " ('but', 'it', 'is'): 17,\n",
       " ('it', 'is', 'returned'): 1,\n",
       " ('is', 'returned', 'she'): 1,\n",
       " ('returned', 'she', ';'): 1,\n",
       " ('she', ';', 'for'): 1,\n",
       " (';', 'for', 'mrs'): 3,\n",
       " ('for', 'mrs', '.'): 4,\n",
       " ('long', 'has', 'just'): 1,\n",
       " ('has', 'just', 'been'): 1,\n",
       " ('just', 'been', 'here'): 1,\n",
       " ('been', 'here', 'and'): 1,\n",
       " ('here', 'and', 'she'): 1,\n",
       " ('and', 'she', 'told'): 2,\n",
       " ('she', 'told', 'me'): 3,\n",
       " ('told', 'me', 'all'): 1,\n",
       " ('me', 'all', 'about'): 1,\n",
       " ('all', 'about', 'it'): 2,\n",
       " ('about', 'it', '.'): 7,\n",
       " ('it', '.', 'mr'): 4,\n",
       " ('.', 'mr', '.'): 33,\n",
       " ('.', 'bennet', 'made'): 2,\n",
       " ('bennet', 'made', 'no'): 2,\n",
       " ('made', 'no', 'answer'): 15,\n",
       " ('no', 'answer', '.'): 8,\n",
       " ('do', 'not', 'you'): 4,\n",
       " ('not', 'you', 'want'): 1,\n",
       " ('you', 'want', 'to'): 2,\n",
       " ('want', 'to', 'know'): 2,\n",
       " ('to', 'know', 'who'): 1,\n",
       " ('know', 'who', 'has'): 1,\n",
       " ('who', 'has', 'taken'): 1,\n",
       " ('has', 'taken', 'it'): 1,\n",
       " ('taken', 'it', '?'): 1,\n",
       " ('it', '?', 'cried'): 1,\n",
       " ('?', 'cried', 'his'): 1,\n",
       " ('cried', 'his', 'wife'): 4,\n",
       " ('his', 'wife', 'impatiently'): 1,\n",
       " ('wife', 'impatiently', '.'): 1,\n",
       " ('long', 'says', 'that'): 1,\n",
       " ('says', 'that', 'netherfield'): 1,\n",
       " ('that', 'netherfield', 'is'): 1,\n",
       " ('netherfield', 'is', 'taken'): 1,\n",
       " ('is', 'taken', 'by'): 1,\n",
       " ('taken', 'by', 'a'): 1,\n",
       " ('by', 'a', 'young'): 3,\n",
       " ('a', 'young', 'man'): 13,\n",
       " ('young', 'man', 'of'): 5,\n",
       " ('man', 'of', 'large'): 2,\n",
       " ('of', 'large', 'fortune'): 2,\n",
       " ('large', 'fortune', 'from'): 1,\n",
       " ('fortune', 'from', 'the'): 1,\n",
       " ('from', 'the', 'north'): 1,\n",
       " ('the', 'north', 'of'): 2,\n",
       " ('north', 'of', 'england'): 2,\n",
       " ('of', 'england', ';'): 2,\n",
       " ('england', ';', 'that'): 1,\n",
       " (';', 'that', 'he'): 4,\n",
       " ('that', 'he', 'came'): 2,\n",
       " ('he', 'came', 'down'): 1,\n",
       " ('came', 'down', 'on'): 1,\n",
       " ('down', 'on', 'monday'): 1,\n",
       " ('on', 'monday', 'in'): 1,\n",
       " ('monday', 'in', 'a'): 1,\n",
       " ('in', 'a', 'chaise'): 1,\n",
       " ('a', 'chaise', 'and'): 2,\n",
       " ('chaise', 'and', 'four'): 2,\n",
       " ('and', 'four', 'to'): 1,\n",
       " ('four', 'to', 'see'): 1,\n",
       " ('to', 'see', 'the'): 7,\n",
       " ('see', 'the', 'place'): 3,\n",
       " ('the', 'place', 'and'): 2,\n",
       " ('place', 'and', 'was'): 1,\n",
       " ('and', 'was', 'so'): 3,\n",
       " ('was', 'so', 'much'): 5,\n",
       " ('so', 'much', 'delighted'): 1,\n",
       " ('much', 'delighted', 'with'): 1,\n",
       " ('delighted', 'with', 'it'): 2,\n",
       " ('with', 'it', 'that'): 1,\n",
       " ('it', 'that', 'he'): 2,\n",
       " ('that', 'he', 'agreed'): 1,\n",
       " ('he', 'agreed', 'with'): 1,\n",
       " ('agreed', 'with', 'mr'): 1,\n",
       " ('with', 'mr', '.'): 14,\n",
       " ('mr', '.', 'morris'): 1,\n",
       " ('.', 'morris', 'immediately'): 1,\n",
       " ('morris', 'immediately', ';'): 1,\n",
       " ('immediately', ';', 'that'): 1,\n",
       " ('he', 'is', 'to'): 2,\n",
       " ('is', 'to', 'take'): 1,\n",
       " ('to', 'take', 'possession'): 1,\n",
       " ('take', 'possession', 'before'): 1,\n",
       " ('possession', 'before', 'michaelmas'): 1,\n",
       " ('before', 'michaelmas', 'and'): 1,\n",
       " ('michaelmas', 'and', 'some'): 1,\n",
       " ('and', 'some', 'of'): 1,\n",
       " ('some', 'of', 'his'): 2,\n",
       " ('of', 'his', 'servants'): 1,\n",
       " ('his', 'servants', 'are'): 1,\n",
       " ('servants', 'are', 'to'): 1,\n",
       " ('are', 'to', 'be'): 4,\n",
       " ('to', 'be', 'in'): 15,\n",
       " ('be', 'in', 'the'): 2,\n",
       " ('in', 'the', 'house'): 5,\n",
       " ('the', 'house', 'by'): 1,\n",
       " ('house', 'by', 'the'): 1,\n",
       " ('by', 'the', 'end'): 2,\n",
       " ('the', 'end', 'of'): 8,\n",
       " ('end', 'of', 'next'): 1,\n",
       " ('of', 'next', 'week'): 1,\n",
       " ('next', 'week', '.'): 2,\n",
       " ('week', '.', 'what'): 1,\n",
       " ('.', 'what', 'is'): 3,\n",
       " ('what', 'is', 'his'): 1,\n",
       " ('is', 'his', 'name'): 1,\n",
       " ('his', 'name', '?'): 1,\n",
       " ('name', '?', 'bingley'): 1,\n",
       " ('?', 'bingley', '.'): 1,\n",
       " ('bingley', '.', 'is'): 1,\n",
       " ('.', 'is', 'he'): 1,\n",
       " ('is', 'he', 'married'): 1,\n",
       " ('he', 'married', 'or'): 1,\n",
       " ('married', 'or', 'single'): 2,\n",
       " ('or', 'single', '?'): 1,\n",
       " ('single', '?', 'oh'): 1,\n",
       " ('?', 'oh', '!'): 6,\n",
       " ('oh', '!', 'a'): 1,\n",
       " ('!', 'a', 'single'): 1,\n",
       " ('single', 'man', 'of'): 1,\n",
       " ('large', 'fortune', ';'): 1,\n",
       " ('fortune', ';', 'four'): 1,\n",
       " (';', 'four', 'or'): 1,\n",
       " ('four', 'or', 'five'): 2,\n",
       " ('or', 'five', 'thousand'): 2,\n",
       " ('five', 'thousand', 'a'): 1,\n",
       " ('thousand', 'a', 'year'): 6,\n",
       " ('a', 'year', '.'): 2,\n",
       " ('what', 'a', 'fine'): 1,\n",
       " ('a', 'fine', 'thing'): 1,\n",
       " ('fine', 'thing', 'for'): 1,\n",
       " ('thing', 'for', 'our'): 1,\n",
       " ('for', 'our', 'girls'): 1,\n",
       " ('our', 'girls', '!'): 1,\n",
       " ('girls', '!', 'how'): 1,\n",
       " ('!', 'how', 'so'): 1,\n",
       " ('how', 'so', '?'): 1,\n",
       " ('how', 'can', 'it'): 1,\n",
       " ('can', 'it', 'affect'): 1,\n",
       " ('it', 'affect', 'them'): 1,\n",
       " ('affect', 'them', '?'): 1,\n",
       " ('them', '?', 'my'): 1,\n",
       " ('?', 'my', 'dear'): 1,\n",
       " ('bennet', 'replied', 'his'): 1,\n",
       " ('replied', 'his', 'wife'): 1,\n",
       " ('his', 'wife', 'how'): 1,\n",
       " ('wife', 'how', 'can'): 1,\n",
       " ('how', 'can', 'you'): 6,\n",
       " ('can', 'you', 'be'): 3,\n",
       " ('you', 'be', 'so'): 2,\n",
       " ('be', 'so', 'tiresome'): 1,\n",
       " ('so', 'tiresome', '!'): 1,\n",
       " ('tiresome', '!', 'you'): 1,\n",
       " ('!', 'you', 'must'): 1,\n",
       " ('you', 'must', 'know'): 3,\n",
       " ('must', 'know', 'that'): 3,\n",
       " ('know', 'that', 'i'): 4,\n",
       " ('that', 'i', 'am'): 16,\n",
       " ('i', 'am', 'thinking'): 2,\n",
       " ('am', 'thinking', 'of'): 2,\n",
       " ('thinking', 'of', 'his'): 1,\n",
       " ('of', 'his', 'marrying'): 2,\n",
       " ('his', 'marrying', 'one'): 1,\n",
       " ('marrying', 'one', 'of'): 1,\n",
       " ('one', 'of', 'them'): 6,\n",
       " ('of', 'them', '.'): 11,\n",
       " ('them', '.', 'is'): 1,\n",
       " ('.', 'is', 'that'): 1,\n",
       " ('is', 'that', 'his'): 1,\n",
       " ('that', 'his', 'design'): 1,\n",
       " ('his', 'design', 'in'): 1,\n",
       " ('design', 'in', 'settling'): 1,\n",
       " ('in', 'settling', 'here'): 1,\n",
       " ('settling', 'here', '?'): 1,\n",
       " ('here', '?', 'design'): 1,\n",
       " ('?', 'design', '!'): 1,\n",
       " ('design', '!', 'but'): 1,\n",
       " ('!', 'but', 'it'): 4,\n",
       " ('it', 'is', 'very'): 10,\n",
       " ('is', 'very', 'likely'): 1,\n",
       " ('very', 'likely', 'that'): 1,\n",
       " ('likely', 'that', 'he'): 1,\n",
       " ('that', 'he', '_may_'): 1,\n",
       " ('he', '_may_', 'fall'): 1,\n",
       " ('_may_', 'fall', 'in'): 1,\n",
       " ('fall', 'in', 'love'): 2,\n",
       " ('in', 'love', 'with'): 17,\n",
       " ('love', 'with', 'one'): 1,\n",
       " ('with', 'one', 'of'): 4,\n",
       " ('of', 'them', 'and'): 2,\n",
       " ('them', 'and', 'therefore'): 1,\n",
       " ('and', 'therefore', 'you'): 1,\n",
       " ('therefore', 'you', 'must'): 1,\n",
       " ('you', 'must', 'visit'): 1,\n",
       " ('must', 'visit', 'him'): 1,\n",
       " ('visit', 'him', 'as'): 1,\n",
       " ('him', 'as', 'soon'): 2,\n",
       " ('as', 'soon', 'as'): 46,\n",
       " ('soon', 'as', 'he'): 8,\n",
       " ('as', 'he', 'comes'): 1,\n",
       " ('he', 'comes', '.'): 1,\n",
       " ('comes', '.', 'i'): 1,\n",
       " ('.', 'i', 'see'): 2,\n",
       " ('i', 'see', 'no'): 2,\n",
       " ('see', 'no', 'occasion'): 2,\n",
       " ('no', 'occasion', 'for'): 5,\n",
       " ('occasion', 'for', 'that'): 1,\n",
       " ('for', 'that', '.'): 3,\n",
       " ('you', 'and', 'the'): 1,\n",
       " ('and', 'the', 'girls'): 4,\n",
       " ('the', 'girls', 'may'): 1,\n",
       " ('girls', 'may', 'go'): 1,\n",
       " ('may', 'go', 'or'): 1,\n",
       " ('go', 'or', 'you'): 1,\n",
       " ('or', 'you', 'may'): 1,\n",
       " ('you', 'may', 'send'): 1,\n",
       " ('may', 'send', 'them'): 1,\n",
       " ('send', 'them', 'by'): 1,\n",
       " ('them', 'by', 'themselves'): 3,\n",
       " ('by', 'themselves', 'which'): 1,\n",
       " ('themselves', 'which', 'perhaps'): 1,\n",
       " ('which', 'perhaps', 'will'): 1,\n",
       " ('perhaps', 'will', 'be'): 1,\n",
       " ('will', 'be', 'still'): 1,\n",
       " ('be', 'still', 'better'): 1,\n",
       " ('still', 'better', 'for'): 1,\n",
       " ('better', 'for', 'as'): 1,\n",
       " ('for', 'as', 'you'): 1,\n",
       " ('as', 'you', 'are'): 2,\n",
       " ('you', 'are', 'as'): 1,\n",
       " ('are', 'as', 'handsome'): 1,\n",
       " ('as', 'handsome', 'as'): 2,\n",
       " ('handsome', 'as', 'any'): 1,\n",
       " ('as', 'any', 'of'): 1,\n",
       " ('any', 'of', 'them'): 5,\n",
       " ('of', 'them', 'mr'): 1,\n",
       " ('them', 'mr', '.'): 3,\n",
       " ('mr', '.', 'bingley'): 60,\n",
       " ('.', 'bingley', 'might'): 1,\n",
       " ('bingley', 'might', 'like'): 1,\n",
       " ('might', 'like', 'you'): 1,\n",
       " ('like', 'you', 'the'): 1,\n",
       " ('you', 'the', 'best'): 1,\n",
       " ('the', 'best', 'of'): 4,\n",
       " ('best', 'of', 'the'): 2,\n",
       " ('of', 'the', 'party'): 6,\n",
       " ('the', 'party', '.'): 3,\n",
       " ('party', '.', 'my'): 1,\n",
       " ('.', 'my', 'dear'): 7,\n",
       " ('my', 'dear', 'you'): 4,\n",
       " ('dear', 'you', 'flatter'): 1,\n",
       " ('you', 'flatter', 'me'): 1,\n",
       " ('flatter', 'me', '.'): 1,\n",
       " ('i', 'certainly', '_have_'): 1,\n",
       " ('certainly', '_have_', 'had'): 1,\n",
       " ('_have_', 'had', 'my'): 1,\n",
       " ('had', 'my', 'share'): 1,\n",
       " ('my', 'share', 'of'): 2,\n",
       " ('share', 'of', 'beauty'): 1,\n",
       " ('of', 'beauty', 'but'): 1,\n",
       " ('beauty', 'but', 'i'): 1,\n",
       " ('but', 'i', 'do'): 2,\n",
       " ('i', 'do', 'not'): 49,\n",
       " ('do', 'not', 'pretend'): 3,\n",
       " ('not', 'pretend', 'to'): 3,\n",
       " ('pretend', 'to', 'be'): 4,\n",
       " ('to', 'be', 'any'): 1,\n",
       " ('be', 'any', 'thing'): 1,\n",
       " ('any', 'thing', 'extraordinary'): 3,\n",
       " ('thing', 'extraordinary', 'now'): 1,\n",
       " ('extraordinary', 'now', '.'): 1,\n",
       " ('when', 'a', 'woman'): 1,\n",
       " ('a', 'woman', 'has'): 2,\n",
       " ('woman', 'has', 'five'): 1,\n",
       " ('has', 'five', 'grown'): 1,\n",
       " ('five', 'grown', 'up'): 1,\n",
       " ('grown', 'up', 'daughters'): 1,\n",
       " ('up', 'daughters', 'she'): 1,\n",
       " ('daughters', 'she', 'ought'): 1,\n",
       " ('she', 'ought', 'to'): 1,\n",
       " ('ought', 'to', 'give'): 1,\n",
       " ('to', 'give', 'over'): 1,\n",
       " ('give', 'over', 'thinking'): 1,\n",
       " ('over', 'thinking', 'of'): 1,\n",
       " ('thinking', 'of', 'her'): 2,\n",
       " ('of', 'her', 'own'): 11,\n",
       " ('her', 'own', 'beauty'): 1,\n",
       " ('own', 'beauty', '.'): 1,\n",
       " ('beauty', '.', 'in'): 1,\n",
       " ('.', 'in', 'such'): 1,\n",
       " ('in', 'such', 'cases'): 2,\n",
       " ('such', 'cases', 'a'): 1,\n",
       " ('cases', 'a', 'woman'): 1,\n",
       " ('woman', 'has', 'not'): 1,\n",
       " ('has', 'not', 'often'): 1,\n",
       " ('not', 'often', 'much'): 1,\n",
       " ('often', 'much', 'beauty'): 1,\n",
       " ('much', 'beauty', 'to'): 1,\n",
       " ('beauty', 'to', 'think'): 1,\n",
       " ('to', 'think', 'of'): 11,\n",
       " ('think', 'of', '.'): 1,\n",
       " ('of', '.', 'but'): 1,\n",
       " ('.', 'but', 'my'): 4,\n",
       " ('but', 'my', 'dear'): 6,\n",
       " ('dear', 'you', 'must'): 1,\n",
       " ('you', 'must', 'indeed'): 1,\n",
       " ('must', 'indeed', 'go'): 1,\n",
       " ('indeed', 'go', 'and'): 1,\n",
       " ('go', 'and', 'see'): 2,\n",
       " ('and', 'see', 'mr'): 1,\n",
       " ('see', 'mr', '.'): 6,\n",
       " ('mr', '.', 'only'): 1,\n",
       " ('.', 'only', 'think'): 1,\n",
       " ('only', 'think', 'what'): 1,\n",
       " ('think', 'what', 'an'): 1,\n",
       " ('what', 'an', 'establishment'): 1,\n",
       " ('an', 'establishment', 'it'): 1,\n",
       " ('establishment', 'it', 'would'): 1,\n",
       " ('it', 'would', 'be'): 26,\n",
       " ('would', 'be', 'for'): 2,\n",
       " ('be', 'for', 'one'): 1,\n",
       " ('for', 'one', 'of'): 2,\n",
       " ('sir', 'william', 'and'): 6,\n",
       " ('william', 'and', 'lady'): 2,\n",
       " ('and', 'lady', 'lucas'): 3,\n",
       " ('lady', 'lucas', 'are'): 1,\n",
       " ('lucas', 'are', 'determined'): 1,\n",
       " ('are', 'determined', 'to'): 3,\n",
       " ('determined', 'to', 'go'): 2,\n",
       " ('to', 'go', 'merely'): 1,\n",
       " ('go', 'merely', 'on'): 1,\n",
       " ('merely', 'on', 'that'): 1,\n",
       " ('on', 'that', 'account'): 1,\n",
       " ('that', 'account', 'for'): 1,\n",
       " ('account', 'for', 'in'): 1,\n",
       " ('for', 'in', 'general'): 1,\n",
       " ('in', 'general', 'you'): 1,\n",
       " ('general', 'you', 'know'): 1,\n",
       " ('you', 'know', 'they'): 1,\n",
       " ('know', 'they', 'visit'): 1,\n",
       " ('they', 'visit', 'no'): 1,\n",
       " ('visit', 'no', 'new'): 1,\n",
       " ('no', 'new', 'comers'): 1,\n",
       " ('new', 'comers', '.'): 1,\n",
       " ('indeed', 'you', 'must'): 2,\n",
       " ('you', 'must', 'go'): 2,\n",
       " ('must', 'go', 'for'): 1,\n",
       " ('go', 'for', 'it'): 1,\n",
       " ('for', 'it', 'will'): 2,\n",
       " ('it', 'will', 'be'): 17,\n",
       " ('will', 'be', 'impossible'): 2,\n",
       " ('be', 'impossible', 'for'): 2,\n",
       " ('impossible', 'for', '_us_'): 1,\n",
       " ('for', '_us_', 'to'): 1,\n",
       " ('_us_', 'to', 'visit'): 1,\n",
       " ('to', 'visit', 'him'): 1,\n",
       " ('visit', 'him', 'if'): 1,\n",
       " ('him', 'if', 'you'): 1,\n",
       " ('if', 'you', 'do'): 9,\n",
       " ('you', 'do', 'not'): 15,\n",
       " ('do', 'not', '.'): 2,\n",
       " ('not', '.', 'you'): 2,\n",
       " ('.', 'you', 'are'): 12,\n",
       " ('you', 'are', 'over'): 1,\n",
       " ('are', 'over', 'scrupulous'): 1,\n",
       " ('over', 'scrupulous', 'surely'): 1,\n",
       " ('scrupulous', 'surely', '.'): 1,\n",
       " ('i', 'dare', 'say'): 24,\n",
       " ('dare', 'say', 'mr'): 2,\n",
       " ('say', 'mr', '.'): 3,\n",
       " ('.', 'bingley', 'will'): 3,\n",
       " ('bingley', 'will', 'be'): 1,\n",
       " ('will', 'be', 'very'): 5,\n",
       " ('be', 'very', 'glad'): 2,\n",
       " ('very', 'glad', 'to'): 4,\n",
       " ('glad', 'to', 'see'): 6,\n",
       " ('to', 'see', 'you'): 4,\n",
       " ('see', 'you', ';'): 1,\n",
       " ('you', ';', 'and'): 7,\n",
       " (';', 'and', 'i'): 39,\n",
       " ('and', 'i', 'will'): 4,\n",
       " ('i', 'will', 'send'): 1,\n",
       " ('will', 'send', 'a'): 1,\n",
       " ('send', 'a', 'few'): 1,\n",
       " ('a', 'few', 'lines'): 4,\n",
       " ('few', 'lines', 'by'): 1,\n",
       " ('lines', 'by', 'you'): 1,\n",
       " ('by', 'you', 'to'): 1,\n",
       " ('you', 'to', 'assure'): 1,\n",
       " ('to', 'assure', 'him'): 3,\n",
       " ('assure', 'him', 'of'): 1,\n",
       " ('him', 'of', 'my'): 1,\n",
       " ('of', 'my', 'hearty'): 1,\n",
       " ('my', 'hearty', 'consent'): 1,\n",
       " ('hearty', 'consent', 'to'): 1,\n",
       " ('consent', 'to', 'his'): 1,\n",
       " ('to', 'his', 'marrying'): 1,\n",
       " ('his', 'marrying', 'which'): 1,\n",
       " ('marrying', 'which', 'ever'): 1,\n",
       " ('which', 'ever', 'he'): 1,\n",
       " ('ever', 'he', 'chuses'): 1,\n",
       " ('he', 'chuses', 'of'): 1,\n",
       " ('chuses', 'of', 'the'): 1,\n",
       " ('of', 'the', 'girls'): 3,\n",
       " ('the', 'girls', ';'): 1,\n",
       " ('girls', ';', 'though'): 1,\n",
       " (';', 'though', 'i'): 5,\n",
       " ('though', 'i', 'must'): 1,\n",
       " ('i', 'must', 'throw'): 1,\n",
       " ('must', 'throw', 'in'): 1,\n",
       " ('throw', 'in', 'a'): 1,\n",
       " ('in', 'a', 'good'): 1,\n",
       " ('a', 'good', 'word'): 1,\n",
       " ('good', 'word', 'for'): 1,\n",
       " ('word', 'for', 'my'): 1,\n",
       " ('for', 'my', 'little'): 1,\n",
       " ('my', 'little', 'lizzy'): 1,\n",
       " ('little', 'lizzy', '.'): 1,\n",
       " ('lizzy', '.', 'i'): 2,\n",
       " ('.', 'i', 'desire'): 2,\n",
       " ('i', 'desire', 'you'): 2,\n",
       " ('desire', 'you', 'will'): 2,\n",
       " ('you', 'will', 'do'): 2,\n",
       " ('will', 'do', 'no'): 1,\n",
       " ('do', 'no', 'such'): 1,\n",
       " ('no', 'such', 'thing'): 2,\n",
       " ('such', 'thing', '.'): 3,\n",
       " ('lizzy', 'is', 'not'): 1,\n",
       " ('is', 'not', 'a'): 4,\n",
       " ('not', 'a', 'bit'): 2,\n",
       " ('a', 'bit', 'better'): 1,\n",
       " ('bit', 'better', 'than'): 1,\n",
       " ('better', 'than', 'the'): 1,\n",
       " ('than', 'the', 'others'): 1,\n",
       " ('the', 'others', ';'): 2,\n",
       " ('others', ';', 'and'): 1,\n",
       " ('and', 'i', 'am'): 13,\n",
       " ('i', 'am', 'sure'): 51,\n",
       " ('am', 'sure', 'she'): 4,\n",
       " ('sure', 'she', 'is'): 1,\n",
       " ('she', 'is', 'not'): 3,\n",
       " ('is', 'not', 'half'): 1,\n",
       " ('not', 'half', 'so'): 1,\n",
       " ('half', 'so', 'handsome'): 1,\n",
       " ('so', 'handsome', 'as'): 2,\n",
       " ('handsome', 'as', 'jane'): 1,\n",
       " ('as', 'jane', 'nor'): 1,\n",
       " ('jane', 'nor', 'half'): 1,\n",
       " ('nor', 'half', 'so'): 1,\n",
       " ('half', 'so', 'good'): 1,\n",
       " ('so', 'good', 'humoured'): 1,\n",
       " ('good', 'humoured', 'as'): 1,\n",
       " ('humoured', 'as', 'lydia'): 1,\n",
       " ('as', 'lydia', '.'): 1,\n",
       " ('but', 'you', 'are'): 2,\n",
       " ('you', 'are', 'always'): 2,\n",
       " ('are', 'always', 'giving'): 1,\n",
       " ('always', 'giving', '_her_'): 1,\n",
       " ('giving', '_her_', 'the'): 1,\n",
       " ('_her_', 'the', 'preference'): 1,\n",
       " ('the', 'preference', '.'): 1,\n",
       " ('preference', '.', 'they'): 1,\n",
       " ('.', 'they', 'have'): 1,\n",
       " ('they', 'have', 'none'): 1,\n",
       " ('have', 'none', 'of'): 1,\n",
       " ('none', 'of', 'them'): 2,\n",
       " ('of', 'them', 'much'): 1,\n",
       " ('them', 'much', 'to'): 1,\n",
       " ('much', 'to', 'recommend'): 1,\n",
       " ('to', 'recommend', 'them'): 2,\n",
       " ('recommend', 'them', 'replied'): 1,\n",
       " ('them', 'replied', 'he'): 1,\n",
       " ('replied', 'he', ';'): 2,\n",
       " ('he', ';', 'they'): 1,\n",
       " (';', 'they', 'are'): 4,\n",
       " ('they', 'are', 'all'): 2,\n",
       " ('are', 'all', 'silly'): 1,\n",
       " ('all', 'silly', 'and'): 1,\n",
       " ('silly', 'and', 'ignorant'): 1,\n",
       " ('and', 'ignorant', 'like'): 1,\n",
       " ('ignorant', 'like', 'other'): 1,\n",
       " ('like', 'other', 'girls'): 1,\n",
       " ('other', 'girls', ';'): 1,\n",
       " ('girls', ';', 'but'): 1,\n",
       " (';', 'but', 'lizzy'): 1,\n",
       " ('but', 'lizzy', 'has'): 1,\n",
       " ('lizzy', 'has', 'something'): 1,\n",
       " ('has', 'something', 'more'): 1,\n",
       " ('something', 'more', 'of'): 2,\n",
       " ('more', 'of', 'quickness'): 1,\n",
       " ('of', 'quickness', 'than'): 1,\n",
       " ('quickness', 'than', 'her'): 1,\n",
       " ('than', 'her', 'sisters'): 2,\n",
       " ('her', 'sisters', '.'): 3,\n",
       " ('sisters', '.', 'mr'): 1,\n",
       " ('mr', '.', 'you'): 3,\n",
       " ('.', 'you', 'take'): 1,\n",
       " ('you', 'take', 'delight'): 1,\n",
       " ('take', 'delight', 'in'): 1,\n",
       " ('delight', 'in', 'vexing'): 1,\n",
       " ('in', 'vexing', 'me'): 1,\n",
       " ('vexing', 'me', '.'): 1,\n",
       " ('you', 'have', 'no'): 3,\n",
       " ('have', 'no', 'compassion'): 1,\n",
       " ('no', 'compassion', 'on'): 1,\n",
       " ('compassion', 'on', 'my'): 2,\n",
       " ('on', 'my', 'poor'): 1,\n",
       " ('my', 'poor', 'nerves'): 2,\n",
       " ('poor', 'nerves', '.'): 2,\n",
       " ('nerves', '.', 'you'): 1,\n",
       " ('.', 'you', 'mistake'): 1,\n",
       " ('you', 'mistake', 'me'): 1,\n",
       " ('mistake', 'me', 'my'): 1,\n",
       " ('me', 'my', 'dear'): 4,\n",
       " ('my', 'dear', '.'): 3,\n",
       " ('they', 'are', 'my'): 1,\n",
       " ('are', 'my', 'old'): 1,\n",
       " ('my', 'old', 'friends'): 1,\n",
       " ('old', 'friends', '.'): 1,\n",
       " ('i', 'have', 'heard'): 5,\n",
       " ('have', 'heard', 'you'): 3,\n",
       " ('heard', 'you', 'mention'): 1,\n",
       " ('you', 'mention', 'them'): 1,\n",
       " ('mention', 'them', 'with'): 1,\n",
       " ('them', 'with', 'consideration'): 1,\n",
       " ('with', 'consideration', 'these'): 1,\n",
       " ('consideration', 'these', 'twenty'): 1,\n",
       " ('these', 'twenty', 'years'): 1,\n",
       " ('twenty', 'years', 'at'): 1,\n",
       " ('years', 'at', 'least'): 1,\n",
       " ('at', 'least', '.'): 2,\n",
       " ('least', '.', 'ah'): 1,\n",
       " ('.', 'ah', '!'): 3,\n",
       " ('ah', '!', 'you'): 1,\n",
       " ('!', 'you', 'do'): 1,\n",
       " ('do', 'not', 'know'): 15,\n",
       " ('not', 'know', 'what'): 7,\n",
       " ('know', 'what', 'i'): 2,\n",
       " ('what', 'i', 'suffer'): 2,\n",
       " ('i', 'suffer', '.'): 1,\n",
       " ('suffer', '.', 'but'): 1,\n",
       " ('.', 'but', 'i'): 5,\n",
       " ('but', 'i', 'hope'): 6,\n",
       " ('i', 'hope', 'you'): 11,\n",
       " ('hope', 'you', 'will'): 8,\n",
       " ('you', 'will', 'get'): 1,\n",
       " ('will', 'get', 'over'): 1,\n",
       " ('get', 'over', 'it'): 2,\n",
       " ('over', 'it', 'and'): 1,\n",
       " ('it', 'and', 'live'): 1,\n",
       " ('and', 'live', 'to'): 2,\n",
       " ('live', 'to', 'see'): 2,\n",
       " ('to', 'see', 'many'): 1,\n",
       " ('see', 'many', 'young'): 1,\n",
       " ('many', 'young', 'men'): 1,\n",
       " ('young', 'men', 'of'): 1,\n",
       " ('men', 'of', 'four'): 1,\n",
       " ('of', 'four', 'thousand'): 1,\n",
       " ('four', 'thousand', 'a'): 1,\n",
       " ('a', 'year', 'come'): 1,\n",
       " ('year', 'come', 'into'): 1,\n",
       " ('come', 'into', 'the'): 2,\n",
       " ('into', 'the', 'neighbourhood'): 1,\n",
       " ('the', 'neighbourhood', '.'): 3,\n",
       " ('neighbourhood', '.', 'it'): 1,\n",
       " ('.', 'it', 'will'): 2,\n",
       " ('will', 'be', 'no'): 1,\n",
       " ('be', 'no', 'use'): 1,\n",
       " ('no', 'use', 'to'): 1,\n",
       " ('use', 'to', 'us'): 1,\n",
       " ('to', 'us', 'if'): 1,\n",
       " ('us', 'if', 'twenty'): 1,\n",
       " ('if', 'twenty', 'such'): 1,\n",
       " ('twenty', 'such', 'should'): 1,\n",
       " ('such', 'should', 'come'): 1,\n",
       " ('should', 'come', 'since'): 1,\n",
       " ('come', 'since', 'you'): 1,\n",
       " ('since', 'you', 'will'): 2,\n",
       " ('you', 'will', 'not'): 14,\n",
       " ('will', 'not', 'visit'): 1,\n",
       " ('not', 'visit', 'them'): 1,\n",
       " ('visit', 'them', '.'): 2,\n",
       " ('them', '.', 'depend'): 1,\n",
       " ('.', 'depend', 'upon'): 1,\n",
       " ('depend', 'upon', 'it'): 6,\n",
       " ('upon', 'it', 'my'): 1,\n",
       " ('it', 'my', 'dear'): 2,\n",
       " ('my', 'dear', 'that'): 2,\n",
       " ('dear', 'that', 'when'): 1,\n",
       " ('that', 'when', 'there'): 2,\n",
       " ('when', 'there', 'are'): 1,\n",
       " ('there', 'are', 'twenty'): 1,\n",
       " ('are', 'twenty', 'i'): 1,\n",
       " ('twenty', 'i', 'will'): 1,\n",
       " ('i', 'will', 'visit'): 2,\n",
       " ('will', 'visit', 'them'): 1,\n",
       " ('visit', 'them', 'all'): 1,\n",
       " ('them', 'all', '.'): 2,\n",
       " ('all', '.', 'mr'): 1,\n",
       " ('.', 'bennet', 'was'): 7,\n",
       " ('bennet', 'was', 'so'): 1,\n",
       " ('was', 'so', 'odd'): 1,\n",
       " ('so', 'odd', 'a'): 1,\n",
       " ('odd', 'a', 'mixture'): 1,\n",
       " ('a', 'mixture', 'of'): 5,\n",
       " ('mixture', 'of', 'quick'): 1,\n",
       " ('of', 'quick', 'parts'): 1,\n",
       " ('quick', 'parts', 'sarcastic'): 1,\n",
       " ('parts', 'sarcastic', 'humour'): 1,\n",
       " ('sarcastic', 'humour', 'reserve'): 1,\n",
       " ('humour', 'reserve', 'and'): 1,\n",
       " ('reserve', 'and', 'caprice'): 1,\n",
       " ('and', 'caprice', 'that'): 1,\n",
       " ('caprice', 'that', 'the'): 1,\n",
       " ('that', 'the', 'experience'): 1,\n",
       " ('the', 'experience', 'of'): 1,\n",
       " ('experience', 'of', 'three'): 1,\n",
       " ('of', 'three', 'and'): 1,\n",
       " ('three', 'and', 'twenty'): 3,\n",
       " ('and', 'twenty', 'years'): 2,\n",
       " ('twenty', 'years', 'had'): 1,\n",
       " ('years', 'had', 'been'): 1,\n",
       " ('had', 'been', 'insufficient'): 1,\n",
       " ('been', 'insufficient', 'to'): 1,\n",
       " ('insufficient', 'to', 'make'): 1,\n",
       " ('to', 'make', 'his'): 4,\n",
       " ('make', 'his', 'wife'): 1,\n",
       " ('his', 'wife', 'understand'): 1,\n",
       " ('wife', 'understand', 'his'): 1,\n",
       " ('understand', 'his', 'character'): 2,\n",
       " ('his', 'character', '.'): 2,\n",
       " ('_her_', 'mind', 'was'): 1,\n",
       " ('mind', 'was', 'less'): 1,\n",
       " ('was', 'less', 'difficult'): 1,\n",
       " ('less', 'difficult', 'to'): 1,\n",
       " ('difficult', 'to', 'develope'): 1,\n",
       " ('to', 'develope', '.'): 1,\n",
       " ('when', 'she', 'was'): 4,\n",
       " ('she', 'was', 'discontented'): 1,\n",
       " ('was', 'discontented', 'she'): 1,\n",
       " ('discontented', 'she', 'fancied'): 1,\n",
       " ('she', 'fancied', 'herself'): 1,\n",
       " ('fancied', 'herself', 'nervous'): 1,\n",
       " ('herself', 'nervous', '.'): 1,\n",
       " ('chapter', 'ii', '.'): 3,\n",
       " ('bennet', 'was', 'among'): 1,\n",
       " ('was', 'among', 'the'): 1,\n",
       " ('among', 'the', 'earliest'): 1,\n",
       " ('the', 'earliest', 'of'): 1,\n",
       " ('earliest', 'of', 'those'): 1,\n",
       " ('of', 'those', 'who'): 2,\n",
       " ('those', 'who', 'waited'): 1,\n",
       " ('who', 'waited', 'on'): 2,\n",
       " ('waited', 'on', 'mr'): 1,\n",
       " ('on', 'mr', '.'): 8,\n",
       " ('.', 'bingley', '.'): 6,\n",
       " ('it', 'was', 'then'): 2,\n",
       " ('was', 'then', 'disclosed'): 1,\n",
       " ('then', 'disclosed', 'in'): 1,\n",
       " ('disclosed', 'in', 'the'): 1,\n",
       " ('in', 'the', 'following'): 1,\n",
       " ('the', 'following', 'manner'): 1,\n",
       " ('following', 'manner', '.'): 1,\n",
       " ('observing', 'his', 'second'): 1,\n",
       " ('his', 'second', 'daughter'): 3,\n",
       " ('second', 'daughter', 'employed'): 1,\n",
       " ('daughter', 'employed', 'in'): 1,\n",
       " ('employed', 'in', 'trimming'): 1,\n",
       " ...}"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def freq_of_freq(train_trigram_count):\n",
    "    freq_of_freq_trigram = {}\n",
    "    for _, count in train_trigram_count.items():\n",
    "        freq_of_freq_trigram[count] = freq_of_freq_trigram.get(count, 0) + 1\n",
    "    return freq_of_freq_trigram\n",
    "\n",
    "\n",
    "unigram_count = PerformNgram(\" \".join(train_sentences), 1)\n",
    "bigram_count = PerformNgram(\" \".join(train_sentences), 2)\n",
    "train_trigram_count = PerformNgram(\" \".join(train_sentences), 3)\n",
    "freq_of_freq_trigram = freq_of_freq(train_trigram_count)\n",
    "train_trigram_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51 2 [[260.95184152]] [[10.03660929]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "def findLinearRegression(freq_of_freq_trigram):\n",
    "    Nc_values = np.array(list(freq_of_freq_trigram.keys())).reshape(-1, 1)  # Features (Nc)\n",
    "    c_values = np.array(list(freq_of_freq_trigram.values())).reshape(-1, 1)  # Target variable (c)\n",
    "    # Fit linear regression model\n",
    "    # Log-transform Nc and c values\n",
    "    log_Nc_values = np.log(Nc_values)\n",
    "    log_c_values = np.log(c_values)\n",
    "    # Fit linear regression model to log-transformed data\n",
    "    model = LinearRegression()\n",
    "    model.fit(log_c_values, log_Nc_values)\n",
    "    return model\n",
    "model = findLinearRegression(freq_of_freq_trigram)\n",
    "#predict model first\n",
    "#Finding modified count for smoothing good turing\n",
    "modified_count = {}\n",
    "for count, Nc in freq_of_freq_trigram.items():\n",
    "    Nc_1 = freq_of_freq_trigram.get(count+1, 0)\n",
    "    if Nc_1 == 0:\n",
    "        # use model now\n",
    "        log_c_new = np.log([[count + 1]])\n",
    "        log_Nc_predicted = model.predict(log_c_new)\n",
    "        Nc_predicted = np.exp(log_Nc_predicted)\n",
    "        c_new = (count + 1) * Nc_predicted / Nc\n",
    "        print(count, Nc, c_new, Nc_predicted)\n",
    "        break\n",
    "    else:\n",
    "        c_new = (count + 1) * Nc_1 / Nc\n",
    "    modified_count[count] = c_new\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('mr', '.', 'bennet'): 4.0, ('that', 'he', 'had'): 2.0, ('as', 'soon', 'as'): 2.0, ('mr', '.', 'bingley'): 2.0, ('i', 'do', 'not'): 2.0, ('i', 'am', 'sure'): 4.0, (';', 'and', 'after'): 16.0, ('of', 'mr', '.'): 2.0, ('as', 'much', 'as'): 16.0, ('the', 'rest', 'of'): 16.0, ('she', 'could', 'not'): 2.0, ('mr', '.', 'darcy'): 2.0, ('and', 'mr', '.'): 2.0, ('could', 'not', 'help'): 16.0, ('the', 'room', '.'): 16.0, ('not', 'to', 'be'): 16.0, (';', 'but', 'she'): 16.0, ('mr', '.', 'collins'): 2.0, ('mr', '.', 'wickham'): 2.0, ('*', '*', '*'): 16.0}\n"
     ]
    }
   ],
   "source": [
    "# Pc = ((count(w1w2w3) + 1) * N(count(w1w2w3) + 1)) / (count(w1w2w3))\n",
    "# if N(count(w1w2w3) + 1) is 0 then \n",
    "#  for all we have to find Zc = N(count(w1w2w3)) / (0.5 * (t - q)) so we can use it later for those whoe \n",
    "# val becomes 0\n",
    "# where t = count(w1w2w3) + 1 and q = count(w1w2w3) - 1\n",
    "# if(count(w1w2w3) + 1 == 0) then t - q = count(w1w2w3) - q\n",
    "# if(count(w1w2w3) + 1 == 0) then t - q = t - count(w1w2w3)\n",
    "# After getting Zc, using linear regressiong to get Pc\n",
    "# predicting Pc with linear log Zc vs log Count of all triagrams\n",
    "\n",
    "# for train finding perplexity\n",
    "Zc_train = {}\n",
    "for trigram, count in train_trigram_count.items():\n",
    "    # smoothing\n",
    "\n",
    "    c_plus_1 = count + 1\n",
    "    c_minus_1 = count - 1\n",
    "    N_c_plus_1 = freq_of_freq_trigram.get(c_plus_1, 0)\n",
    "    N_c = freq_of_freq_trigram.get(count, 0)\n",
    "    if N_c_plus_1 == 0:\n",
    "        p = c_plus_1\n",
    "        q = c_minus_1\n",
    "        if train_trigram_count.get(p, 0) != 0 and train_trigram_count.get(q, 0) != 0:\n",
    "            Zc = N_c / (0.5 * (p - q))\n",
    "        elif train_trigram_count.get(q, 0) != 0 :\n",
    "            Zc = N_c / (0.5 * (p - count))\n",
    "        elif train_trigram_count.get(p, 0) == 0:\n",
    "            Zc = N_c / (0.5 * (count - q))\n",
    "        else:\n",
    "            Zc = N_c / (0.5 * (count))\n",
    "        Zc_train[trigram] = Zc\n",
    "# go for testing part\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def good_turing_smoothing(trigram, train_trigram_count,freq_of_freq_trigram):\n",
    "    if train_trigram_count.get(trigram, 0) == 0:\n",
    "        c_plus_1 = 1\n",
    "        c_minus_1 = 0\n",
    "        N_c_plus_1 = freq_of_freq_trigram.get(c_plus_1, 0)\n",
    "        N_c = len(train_trigram_count)\n",
    "        return N_c_plus_1/N_c\n",
    "    else:\n",
    "\n",
    "    return 555\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def perplexity_good_turing(sentence, train_trigram_count, freq_of_freq_trigram):\n",
    "    tokens = tokenize(sentence)\n",
    "        # in this tuple add <start> <start> at the start of the sentence and <end> at the end of the sentence\n",
    "    tokens = ('<START>', '<START>',) + tuple(tokens) + ('<END>',)\n",
    "    # break  \n",
    "    log_probability_sum = 0.0\n",
    "    trigram_count = 0\n",
    "    for i in range(len(tokens)-3):\n",
    "        trigram = tuple(tokens[i:i+3])\n",
    "        trigram_count += 1\n",
    "        temp_prob = math.log(good_turing_smoothing(trigram, train_trigram_count,freq_of_freq_trigram))\n",
    "        # log_probability_sum += math.log(linear_interpolation(trigram, unigram_prob, bigram_prob, trigram_prob))\n",
    "        log_probability_sum += temp_prob\n",
    "\n",
    "    sentence_perplexity = math.exp(-(log_probability_sum / trigram_count))\n",
    "    return sentence_perplexity\n",
    "\n",
    "# Now I have to find the perplexity\n",
    "test_perplexity = {}#perplexity for each sentence\n",
    "for sentence in test_sentences:\n",
    "    test_perplexity[sentence] = perplexity_good_turing(sentence, train_trigram_count,freq_of_freq_trigram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[127], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m sentence_probability \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m train_sentences:\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;66;03m# sentence = \"finding the probability of each sentence\"\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m     sentence_probability[sentence] \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_trigram_probability\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrigram_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbigram_count\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[127], line 16\u001b[0m, in \u001b[0;36mcalculate_trigram_probability\u001b[1;34m(sentence, trigram_counts, bigram_counts)\u001b[0m\n\u001b[0;32m     14\u001b[0m word \u001b[38;5;241m=\u001b[39m padded_sentence[i]\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Get counts from the training data\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m trigram_count \u001b[38;5;241m=\u001b[39m \u001b[43mtrigram_counts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m(trigram, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     17\u001b[0m bigram_count \u001b[38;5;241m=\u001b[39m bigram_counts\u001b[38;5;241m.\u001b[39mget(bigram, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bigram_count \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "# def calculate_trigram_probability(sentence, trigram_counts, bigram_counts):\n",
    "#     # Tokenize the sentence into words\n",
    "#     # Pad the sentence with start and end tokens\n",
    "#     sentence = (3-1)*\"<START> \"+ sentence\n",
    "#     # Initialize the probability\n",
    "#     probability = 1.0\n",
    "#     padded_sentence = sentence.split(' ')\n",
    "#     # Calculate the probability of each word given its preceding bigram\n",
    "#     for i in range(4, len(padded_sentence)):\n",
    "#         trigram = (padded_sentence[i-2:i+1])\n",
    "#         bigram = trigram[:2]\n",
    "#         trigram = ' '.join(trigram)\n",
    "#         bigram = ' '.join(bigram)\n",
    "#         word = padded_sentence[i]\n",
    "#         # Get counts from the training data\n",
    "#         trigram_count = trigram_counts.get(trigram, 0)\n",
    "#         bigram_count = bigram_counts.get(bigram, 0)\n",
    "#         if bigram_count == 0:\n",
    "#             return 0\n",
    "#         probability_word = trigram_count / bigram_count\n",
    "#         probability *= probability_word   \n",
    "    \n",
    "#     return probability\n",
    "\n",
    "# # Example usage:\n",
    "# sentence_probability = {}\n",
    "# for sentence in train_sentences:\n",
    "#     # sentence = \"finding the probability of each sentence\"\n",
    "#     sentence_probability[sentence] = calculate_trigram_probability(sentence, trigram_count, bigram_count)\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now finding smoothing for each sentence\n",
    "If that is present then ditectly find the value otherwise have to find Zc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'project', 'gutenberg', 'ebook', 'pride', 'and', 'prejudice', 'by', 'jane', 'austen', 'edited', 'by', 'r', '.']\n"
     ]
    }
   ],
   "source": [
    "# Pc = ((count(w1w2w3) + 1) * N(count(w1w2w3) + 1)) / (count(w1w2w3))\n",
    "# if N(count(w1w2w3) + 1) is 0 then \n",
    "#  for all we have to find Zc = N(count(w1w2w3)) / (0.5 * (t - q)) so we can use it later for those whoe \n",
    "# val becomes 0\n",
    "# where t = count(w1w2w3) + 1 and q = count(w1w2w3) - 1\n",
    "# if(count(w1w2w3) + 1 == 0) then t - q = count(w1w2w3) - q\n",
    "# if(count(w1w2w3) + 1 == 0) then t - q = t - count(w1w2w3)\n",
    "# After getting Zc, using linear regressiong to get Pc\n",
    "# predicting Pc with linear log Zc vs log Count of all triagrams\n",
    "\n",
    "trigram_count\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class N_Gram_Model:\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "    def read_file(file_path):\n",
    "        with open(file_path, 'r') as f:\n",
    "            text = f.read()\n",
    "        return text\n",
    "    def test_train_split(corpus, n):\n",
    "        # remove new line\n",
    "        corpus = corpus.replace('\\n', ' ')\n",
    "        # split into sentences\n",
    "        sentences = re.split(r'(?<=[.!?]) +', corpus)\n",
    "        test_sentences = random.sample(sentences, n)\n",
    "        train_sentences = [sentence for sentence in sentences if sentence not in test_sentences]\n",
    "        return test_sentences, train_sentences\n",
    "    \n",
    "    def train():\n",
    "        pass\n",
    "    def test():\n",
    "        pass\n",
    "    def save():\n",
    "        pass\n",
    "    def load(): \n",
    "        pass\n",
    "    def perplexity():\n",
    "        # it is required to calculate perplexity of train and test data\n",
    "        pass\n",
    "    def generate():\n",
    "        # genearate a sentence\n",
    "        # use probability calculated in train to generate next word\n",
    "        pass\n",
    "\n",
    "    def evaluate():\n",
    "        # evaluate the model\n",
    "        # calculate perplexity of train and test data\n",
    "        pass\n",
    "\n",
    "    def good_turing():\n",
    "        # implement good turing smoothing\n",
    "        pass\n",
    "\n",
    "    def interpolation():\n",
    "        # implement interpolation smoothing\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# from nltk.tokenize import RegexpTokenizer\n",
    "# from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import random\n",
    "import sys\n",
    "import math\n",
    "# nltk.download('punkt')\n",
    "\n",
    "#maps \n",
    "ngram_context_counter = {}\n",
    "ngram_continue_counter = {}\n",
    "words_preceding_ngram = {}\n",
    "words_prec_and_infront_ngram = {}\n",
    "unique_words_preceding_ngram = {}\n",
    "ngram_continue_counter_dict = {}\n",
    "\n",
    "#helper funcitons \n",
    "def test_train_split(text ,n):\n",
    "    text = re.sub(r\"\\n\", ' ', text)\n",
    "    sentences = re.split(r'(?<=[.!?]) +', text)\n",
    "    test_sentences = random.sample(sentences, n)\n",
    "    train_sentences = [sentence for sentence in sentences if sentence not in test_sentences]\n",
    "    return test_sentences, train_sentences\n",
    "\n",
    "def tokenize(text):\n",
    "    url_re = \"(http|ftp|https):\\/\\/([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:\\/~+#-]*[\\w@?^=%&\\/~+#-])\"\n",
    "    mention_re = \"@\\w+\"\n",
    "    hastag_re = \"#[a-z0-9_]+\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(url_re, '<url> ', text)\n",
    "    text = re.sub(hastag_re, '<hashtag> ', text)\n",
    "    text = re.sub(mention_re, '<mention> ', text)\n",
    "    tokens = re.findall(r'\\b\\w+|[^\\s\\w<>]+|<\\w+>', text)\n",
    "    return tokens\n",
    "\n",
    "def generate_ngrams(tokens, n):\n",
    "    tokens = (n-1)*['<START>']+tokens\n",
    "    print(tokens)\n",
    "    ngrams = [(tuple(tokens[i-p-1] for p in reversed(range(n-1))), tokens[i]) for i in range(n-1, len(tokens))]\n",
    "    return ngrams\n",
    "\n",
    "def gen_context_counter(ngrams):\n",
    "    ngram_context = {}\n",
    "    ngram_counter = {}\n",
    "\n",
    "    for ngram in ngrams:\n",
    "        if ngram in ngram_counter:\n",
    "            ngram_counter[ngram] += 1\n",
    "        else:\n",
    "            ngram_counter[ngram] = 1\n",
    "        \n",
    "        prev_words, target_word = ngram\n",
    "        \n",
    "        if prev_words in ngram_context:\n",
    "            ngram_context[prev_words].append(target_word)\n",
    "        else:\n",
    "            ngram_context[prev_words] = [target_word]\n",
    "    \n",
    "    return ngram_context, ngram_counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
