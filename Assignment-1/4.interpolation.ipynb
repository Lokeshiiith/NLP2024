{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "def test_train_split(corpus, n):\n",
    "    # remove new line\n",
    "    corpus = corpus.replace('\\n', ' ')\n",
    "    # split into sentences\n",
    "    sentences = re.split(r'(?<=[.!?]) +', corpus)\n",
    "    test_sentences = random.sample(sentences, n)\n",
    "    train_sentences = [sentence for sentence in sentences if sentence not in test_sentences]\n",
    "    return test_sentences, train_sentences\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    url_pattern1 = r\"(http|ftp|https):\\/\\/([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:\\/~+#-]*[\\w@?^=%&\\/~+#-])\"\n",
    "    url_pattern2 = r'www\\.[^\\s\\.]+(?:\\.[^\\s\\.]+)*(?:[\\s\\.]|$)'\n",
    "    email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
    "    mention_pattern = \"@\\w+\"\n",
    "    hastag_pattern = \"#[a-z0-9_]+\"\n",
    "    normal_pattern = \"[a-zA-Z]+\"\n",
    "    number_pattern = \"[0-9]+\"\n",
    "    tokens = []\n",
    "    text = text.lower()\n",
    "    text = re.sub(url_pattern1, '<URL> ', text)\n",
    "    text = re.sub(url_pattern2, '<URL> ', text)\n",
    "    text = re.sub(email_pattern, '<MAILID> ', text)\n",
    "    text = re.sub(hastag_pattern, '<HASHTAG> ', text)\n",
    "    text = re.sub(mention_pattern, '<MENTION> ', text)\n",
    "    text = re.sub(number_pattern, '<NUM> ', text)\n",
    "    tokens = re.findall(r'\\b\\w+|[^\\s\\w<>]+|<\\w+>', text)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "coupus_path = './corpus'\n",
    "corpus1 = \"./corpus/Pride and Prejudice - Jane Austen.txt\"\n",
    "corpus2 = \"./corpus/Ulysses  James Joyce.txt\"\n",
    "with open(corpus1, 'r', encoding='utf-8') as f:\n",
    "    text1 = f.read()\n",
    "test_sentences, train_sentences = test_train_split(text1, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_cleaning(text):\n",
    "    # remove comma, extra spaces, and punctuations\n",
    "    text = re.sub(r'[,!?;-]+', '', text)\n",
    "    if text.endswith('.'):\n",
    "            text = text[:-1]#removing last dot also\n",
    "    return text\n",
    "def PerformNgram(corpus, n):\n",
    "    pattern = \"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s\"\n",
    "    list_sentences = re.split(pattern, corpus)\n",
    "    ngrams = {}\n",
    "    for sentence in list_sentences:\n",
    "        tokens = tokenize(sentence)\n",
    "        # sentence = (n-1)*\"<START> \"+ sentence\n",
    "        for i in range(len(tokens)-n+1):\n",
    "            temp = tuple(tokens[j] for j in range(i, i+n))  # Convert list to tuple\n",
    "            if temp in ngrams:\n",
    "                ngrams[temp] += 1\n",
    "            else:\n",
    "                ngrams[temp] = 1\n",
    "            \n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_count = PerformNgram(\" \".join(train_sentences), 1)\n",
    "bigram_count = PerformNgram(\" \".join(train_sentences), 2)\n",
    "trigram_count = PerformNgram(\" \".join(train_sentences), 3)\n",
    "# # saving the ngrams\n",
    "# with open('unigram_count.txt', 'w') as f:\n",
    "#     for key, value in unigram_count.items():\n",
    "#         f.write('%s:%s\\n' % (key, value))\n",
    "# with open('bigram_count.txt', 'w') as f:\n",
    "#     for key, value in bigram_count.items():\n",
    "#         f.write('%s:%s\\n' % (key, value))\n",
    "# with open('trigram_count.txt', 'w') as f:\n",
    "#     for key, value in trigram_count.items():\n",
    "#         f.write('%s:%s\\n' % (key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I have to find all probabilities\n",
    "unigram_prob = {}\n",
    "bigram_prob = {}\n",
    "trigram_prob = {}\n",
    "for key, value in unigram_count.items():\n",
    "    unigram_prob[key] = value / len(unigram_count)\n",
    "# print(unigram_prob)\n",
    "\n",
    "# probabilty of bigram\n",
    "# p(w2|w1) = count(w1, w2)/count(w1)\n",
    "for bigram, count in bigram_count.items():\n",
    "    w1 = bigram[0]\n",
    "    w1_token = (w1,)\n",
    "    bigram_prob[bigram] = count / unigram_count[w1_token]\n",
    "# with open('bigram_prob.txt', 'w') as f:\n",
    "#     for key, value in bigram_prob.items():\n",
    "#         f.write('%s:%s\\n' % (key, value))\n",
    "\n",
    "# similarly for trigram\n",
    "for trigram, w1_w2_w3 in trigram_count.items():\n",
    "    w1_w2_token = (trigram[0], trigram[1],)\n",
    "    trigram_prob[trigram] = w1_w2_w3 / bigram_count[w1_w2_token]\n",
    "# with open('trigram_prob.txt', 'w') as f:\n",
    "#     for key, value in trigram_prob.items():\n",
    "#         f.write('%s:%s\\n' % (key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_interpolation(trigram, unigram_prob, bigram_prob, trigram_prob):\n",
    "    lambda1 = 0.4\n",
    "    lambda2 = 0.3\n",
    "    lambda3 = 0.3\n",
    "    unigram_probability = unigram_prob.get(trigram[-1], 0)\n",
    "    bigram_probability = bigram_prob.get(tuple(trigram[-2:]), 0)\n",
    "    trigram_probability = trigram_prob.get(tuple(trigram[-3:]), 0)\n",
    "\n",
    "    unigram_probability = lambda3 * unigram_probability  \n",
    "    if unigram_probability == 0:\n",
    "        unigram_probability = 0.00001\n",
    "    \n",
    "    trigram_probability = lambda1 * trigram_probability\n",
    "    if trigram_probability == 0:\n",
    "        trigram_probability = 1/len(trigram_prob)\n",
    "\n",
    "    bigram_probability = lambda2 * bigram_probability\n",
    "    if bigram_probability == 0:\n",
    "        bigram_probability = 1/len(bigram_prob)\n",
    "\n",
    "    interpolated_prob = trigram_probability + bigram_probability + unigram_probability\n",
    "    return interpolated_prob\n",
    "\n",
    "\n",
    "test_trigram_count = PerformNgram(\" \".join(test_sentences), 3)\n",
    "with open('test_trigram_cnt.txt', 'w') as f:\n",
    "    for key, value in test_trigram_count.items():\n",
    "        f.write('%s:%s\\n' % (key, value))\n",
    "\n",
    "test_trigram_prob = {}\n",
    "for trigram, count in test_trigram_count.items():\n",
    "    test_trigram_prob[trigram] = linear_interpolation(trigram, unigram_prob, bigram_prob, trigram_prob)\n",
    "    # break\n",
    "with open('test_trigram_prob.txt', 'w') as f:\n",
    "    for key, value in test_trigram_prob.items():\n",
    "        f.write('%s:%s\\n' % (key, value))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <!-- Now finding perplexity -->\n",
    "import math\n",
    "def perplexity(sentence, unigram_prob, bigram_prob, trigram_prob):\n",
    "    tokens = tokenize(sentence)\n",
    "        # in this tuple add <start> <start> at the start of the sentence and <end> at the end of the sentence\n",
    "    tokens = ('<START>', '<START>',) + tuple(tokens) + ('<END>',)\n",
    "    # break  \n",
    "    log_probability_sum = 0.0\n",
    "    trigram_count = 1\n",
    "    for i in range(len(tokens)-3):\n",
    "        trigram = tuple(tokens[i:i+3])\n",
    "        trigram_count += 1\n",
    "        temp_prob = math.log(linear_interpolation(trigram, unigram_prob, bigram_prob, trigram_prob))\n",
    "        # log_probability_sum += math.log(linear_interpolation(trigram, unigram_prob, bigram_prob, trigram_prob))\n",
    "        log_probability_sum += temp_prob\n",
    "\n",
    "    sentence_perplexity = math.exp(-(log_probability_sum / trigram_count))\n",
    "    return sentence_perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perplexity for train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I have to find the perplexity\n",
    "train_perplexity = {}#perplexity for each sentence\n",
    "for sentence in train_sentences:\n",
    "    token = tokenize(sentence)\n",
    "    sentence = ' '.join(token)\n",
    "    train_perplexity[sentence] = perplexity(sentence, unigram_prob, bigram_prob, trigram_prob)\n",
    "# Find avg perplexity\n",
    "avg_perplexity = sum(train_perplexity.values()) / len(train_perplexity)\n",
    "with open('./2022201041_LM4_train-perplexity.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write('Average Perplexity: %s\\n\\n' % avg_perplexity)\n",
    "    for key, value in train_perplexity.items():\n",
    "        f.write('%s : %s\\n' % (key, value))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each sentence in test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perplexity for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I have to find the perplexity\n",
    "test_perplexity = {}#perplexity for each sentence\n",
    "for sentence in test_sentences:\n",
    "    token = tokenize(sentence)\n",
    "    sentence = ' '.join(token)\n",
    "    test_perplexity[sentence] = perplexity(sentence, unigram_prob, bigram_prob, trigram_prob)\n",
    "avg_perplexity = sum(test_perplexity.values())/ len(test_perplexity)\n",
    "with open('2022201041_LM4_test-perplexity.txt', 'w') as f:\n",
    "    f.write('Average Perplexity: %s\\n\\n' % avg_perplexity)\n",
    "    for key, value in test_perplexity.items():\n",
    "        f.write('%s : %s\\n' % (key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = 'An apple a day keeps the doctor'\n",
    "token = tokenize(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('the', 'doctor')"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = token[-2:]\n",
    "w1 = tuple(w1,)\n",
    "w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability: 0.1350146425255339, Word: more\n",
      "Probability: 0.1337611606313835, Word: used\n",
      "Probability: 0.1337611606313835, Word: thereby\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "st = 'a carat character is'\n",
    "token = tokenize(st)\n",
    "w1 = token[-2:]\n",
    "w1 = tuple(w1,)\n",
    "k = 3\n",
    "# Initialize a dictionary to store word probabilities\n",
    "word_probabilities = {}\n",
    "for sentence in train_sentences:\n",
    "    # Split the sentence into words\n",
    "    words = tokenize(sentence)\n",
    "    \n",
    "    # Extract the last two words and convert them into a tuple\n",
    "    for eachword in words:\n",
    "        w = (w1,eachword)\n",
    "        w = w[0] + (w[1],)\n",
    "        prob = linear_interpolation(w,unigram_prob, bigram_prob, trigram_prob)\n",
    "        word_probabilities[eachword] = prob\n",
    "# Print the list of tuples\n",
    "sorted_word_probabilities = sorted(word_probabilities.items(), key=lambda x: x[1], reverse=True)\n",
    "for word, prob in sorted_word_probabilities[:k]:\n",
    "    print(f\"Probability: {prob}, Word: {word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability: 0.00040028696419796574, Word Tuple: (',', 'and', 'wondered')\n",
      "Probability: 0.00040028696419796574, Word Tuple: (',', 'and', 'wretched')\n",
      "Probability: 0.00040028696419796574, Word Tuple: (',', 'and', 'written')\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "min_heap = []\n",
    "\n",
    "# Iterate through each sentence in train_sentences\n",
    "for sentence in train_sentences:\n",
    "    # Split the sentence into words\n",
    "    words = tokenize(sentence)\n",
    "    \n",
    "    # Iterate through each word in the sentence\n",
    "    for i in range(len(words)):\n",
    "        # Extract the last two words and current word\n",
    "        if i >= 2:\n",
    "            w = tuple(words[i-2:i+1])\n",
    "            # Calculate probability using linear interpolation\n",
    "            prob = linear_interpolation(w, unigram_prob, bigram_prob, trigram_prob)\n",
    "            # Push the probability and word tuple into the min heap\n",
    "            # Negate the probability to use min heap as max heap\n",
    "            heapq.heappush(min_heap, (-prob, w))\n",
    "            # If the heap size exceeds k, pop the smallest element\n",
    "            if len(min_heap) > k:\n",
    "                heapq.heappop(min_heap)\n",
    "\n",
    "# Now min_heap contains the k tuples with the highest probabilities\n",
    "# You can iterate through the heap to access the top k tuples\n",
    "# To access the probability and word tuple, you can negate the probability again\n",
    "# to restore the original positive value\n",
    "\n",
    "# Example of accessing the top k tuples\n",
    "for neg_prob, word_tuple in min_heap:\n",
    "    prob = -neg_prob\n",
    "    print(f\"Probability: {prob}, Word Tuple: {word_tuple}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
